[{"authors":["admin"],"categories":null,"content":"Vlad Roubtsov works on stochastic and simulation-based optimization problems at a large retailer. He leverages his cross-disciplinary background (physics, math, computer science) to bridge quantitative research with software development.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"258be61fc6f4ec5204c804c5b2f71116","permalink":"/authors/vlad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vlad/","section":"authors","summary":"Vlad Roubtsov works on stochastic and simulation-based optimization problems at a large retailer. He leverages his cross-disciplinary background (physics, math, computer science) to bridge quantitative research with software development.","tags":null,"title":"Vlad Roubtsov","type":"authors"},{"authors":null,"categories":null,"content":" Introduction and welcome Greetings! I work on stochastic optimization and simulation problems and happen to have extensive background in both \u0026ldquo;high productivity\u0026rdquo; and \u0026ldquo;high performance\u0026rdquo; programming languages. I had my first peek at Julia a few years ago. The concept was great and the new language certainly had a real, uncrowded niche to fit in: something that promised to be both high productivity and high performance. But right then it was brand new and the ecosystem just didn\u0026rsquo;t feel organized or stable enough for \u0026ldquo;production use\u0026rdquo;.\nA few years later, Julia v1.0 milestone has been achieved, there is an IDE or two, university classes are abandoning MATLAB/octave in favor of Julia, and new Julia books are coming out with increasing frequency. Now it appears to me that Julia is much more mature and ready for another, more thorough, examination. For this reason, I invite you to take part in this study group.\nPurpose What do we plan to get out of this study? A couple of things, at least:\n Julia does not look like a radically new programming language to me. Rather, it appears to combine many of the best and most modern aspects of language design in order to support productive scientific programming. I would us like to understand the main paradigms in Julia so that we can judge where and when it would be the most effective choice for research work.\n As a component of overall productivity, I care about language performance in both interactive and parallel/distributed settings. I would like us to put Julia speed claims to the test.\n Julia is still quite new (v1.0 just shipped in 2018) and good study material is a little hard to come by. We could look at some existing open-source tools for operations research and machine learning as examples of working Julia projects, and perhaps prototype a few of our own in the process.\n  Study agenda (open to feedback) At a high level, these are the topics I\u0026rsquo;d like us to cover (not necessarily in the order shown and subject to feedback and suggestions from the group):\n language design and capabilities: core types, generic functions/multiple dispatch, metaprogramming, parallel/distributed programming, etc. general purpose tooling: IDE, plotting, debugging, profiling, etc. native support and tools for OR, ML, and statistics: dataframes, built-in linear algebra, JuMP, JuML, etc.  Each week I would us like to formulate a practical question or need and investigate a solution (or lack thereof) within the Julia ecosystem, at which point I will document it here. Some example questions off the top of my head are:\n What\u0026rsquo;s this I hear about Julia being \u0026ldquo;fast\u0026rdquo;? How does it compare with Python or R? How about C++? I have an optimization problem that needs a custom algorithm not covered by off-the-shelf solvers \u0026ndash; should I consider a (pure) Julia implementation? Conversely, I have a bog-standard LP model \u0026ndash; is there still an advantage to working with it in Julia? How do I develop a Julia package, or, more generally, have workflow that\u0026rsquo;s good for research that I want to share with colleagues? Julia is marketed as good for parallel computing \u0026ndash; what\u0026rsquo;s the reality of this claim? \u0026hellip; and so on, as long as there are useful topics to explore.  Prerequisites I think it would be helpful to have some experience with one or two of the languages that Julia aims to (will eventually?) replace: R, Matlab, Python, and others of that ilk. Julia also borrows from JIT\u0026rsquo;ed languages like Java, so familiarity with those can only help.\n","date":1569024000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1569024000,"objectID":"7ee0ab098699d251db43779d8f3524d3","permalink":"/tutorials/study_julia_with_me/","publishdate":"2019-09-21T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/","section":"tutorials","summary":"Ongoing study of topics from language design, operations research, and computer/data sciences.","tags":null,"title":"Study Julia with me","type":"docs"},{"authors":null,"categories":null,"content":" Suggested software setup I have been using Julia successfully on MacOS and Linux (Fedora). I would suggest starting with a JuliaPro installer for this study. I\u0026rsquo;ve kept a few notes about my install process:\n I\u0026rsquo;ve been using v1.2 so far, both REPL and Juno \u0026ldquo;IDE\u0026rdquo;.\n on MacOS you will need to have XCode tools installed, see the install guide on Linux the install guide mentions needing these libs (install via yum/dnf/apt-get/etc, I only needed to add xclip):\nxclip libXScrnSaver   apparently, starting with v1.2 what\u0026rsquo;s included appears to have changed: to minimize the installer size the \u0026ldquo;curated list of packages\u0026rdquo; are no longer part of the download itself. They can be installed via the usual Pgk commands. What\u0026rsquo;s different for \u0026ldquo;curated\u0026rdquo; packages is that the install will be configured to use Julia Computing\u0026rsquo;s github repo1 so that only the supposedly tested versions are available. Using Julia Computing\u0026rsquo;s repo will require authenticating to get their token. Keep this in mind if you plan to play with packages outside of their supported list.\n for REPL you can use Juno\u0026rsquo;s \u0026ldquo;REPL\u0026rdquo; tab. I find that doing so gives me experince similar to that of RStudio (a good thing):\n  { width=80% }\n if you prefer not to rely on the IDE (which is not that great right now) and use REPL (which is very functional), you can use julia binary from your JuliaPro distribution:  MacOS: /Applications/JuliaPro-\u0026lt;version\u0026gt;.app/Contents/Resources/julia/Contents/Resources/julia/bin/julia Linux: \u0026lt;install dir\u0026gt;/Julia/bin/julia   FAQs and tips for fellow Julia explorers General  if you\u0026rsquo;re working with custom types, it is recommended to keep work code in a module (which could be as simple as keeping it inside a module MyModule ... end block) even for one-off stuff: onlike, say, R or Python Julia currently does not allow updating type definitions within a session without restarting it. It seems to have gone through several iterations of addressing this workflow need, but right now re-include()ing a module seems to be what\u0026rsquo;s guaranteed to work.\n there is also Revise.jl (which I haven\u0026rsquo;t tried yet) see this discussion for more color  another reason for working inside a module is because code will run faster (it will be JIT\u0026rsquo;ed sooner)\n  Juno Juno is basically a few packages inside Atom. It is not quite an \u0026ldquo;IDE\u0026rdquo; at this point. Visual Studio Code might be a reasonable alternative, but in my experiments VSC had trouble with Julia v1.2. Juno comes bundled with a julia build from the same entity.\n as a result of being a collection of Atom packages, some things you might want to tweak in the UI could be dispersed over multiple places. For example:  \u0026ldquo;Julia Client\u0026rdquo; package:  you may wish to choose positioning of various tabs: Workspace, Documentation, Plots, REPL, etc  \u0026ldquo;tool-bar\u0026rdquo; package:  you might want to opt for smaller icons  \u0026ldquo;tree-view\u0026rdquo; package:  \u0026ldquo;Always Open Existing\u0026rdquo;, \u0026ldquo;Auto Reveal\u0026rdquo; settings might be of interest    Books, other resources There is a constantly growing list of resources at https://julialang.org. I list below some resources that I\u0026rsquo;ve either used myself or that seemed to stand out from the rest.\nbooks about or based on Julia v1.0+:  \u0026ldquo;Think Julia: How to Think Like a Computer Scientist\u0026rdquo; by Ben Lauwens and Allen Downey.  this is the only \u0026ldquo;pure computer science\u0026rdquo; Julia book in my list.  \u0026ldquo;A Deep Introduction to Julia for Data Science and Scientific Computing\u0026rdquo; by Chris Rackauckas.  the author is very active in Julia space; this material seems very good (if you\u0026rsquo;re ok with notebooks).  \u0026ldquo;Statistics with Julia: Fundamentals for Data Science, Machine Learning and Artificial Intelligence\u0026rdquo; by Hayden Klok and Yoni Nazarathy (2019 draft PDF free from the authors).  based on a statistics course at the University of Queensland; Julia crash course in Chapter 1 and a handy \u0026ldquo;How-to\u0026rdquo; in Appendix A.  \u0026ldquo;Julia Programming for Operations Research, 2nd ed.\u0026rdquo; by Changhyun Kwon.  another Julia crash course chapter; JuMP workship.   reference docs:  \u0026ldquo;Julia 1.2 Documentation\u0026rdquo;, HTML and PDF. \u0026ldquo;Introducing Julia\u0026rdquo; Wikibook, a nice complement to the language manual.  video lectures, talks:  \u0026ldquo;Intro to Julia 1.0\u0026rdquo; by Jane Herriman of Julia Computing. Videos from JuliaCon 2019 \u0026ndash; some good stuff, particularly by Julia creators.  online practice:  Julia Box from Julia Computing has a free plan. Julia exercism track looks like a good collection of exercises   Julia packages are maintained as git repos. ^   ","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569196800,"objectID":"e744053bcdda3203a21dab248ba3b320","permalink":"/tutorials/study_julia_with_me/structure/","publishdate":"2019-09-23T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/structure/","section":"tutorials","summary":"Suggested software setup I have been using Julia successfully on MacOS and Linux (Fedora). I would suggest starting with a JuliaPro installer for this study. I\u0026rsquo;ve kept a few notes about my install process:\n I\u0026rsquo;ve been using v1.2 so far, both REPL and Juno \u0026ldquo;IDE\u0026rdquo;.\n on MacOS you will need to have XCode tools installed, see the install guide on Linux the install guide mentions needing these libs (install via yum/dnf/apt-get/etc, I only needed to add xclip):","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Every once in a while, you need to try an algorithm that’s “custom enough” that it must be coded directly in your “research language”. Perhaps the existing “high-performance” libs don’t support your desired variant or it’s too much setup work switching to another language just to prototype a quick one-off idea. This an instance of the “two language problem” that’s been the reality of technical computing for decades.\nUsers of R, Python, and similar “high-productivity” environments know that native control flow constructs (for-loops, etc) in those languages kill performance unless they can be expressed in “vectorized” form. One big attraction of Julia to me is that it claims to fix this inconvenience. Let’s have a look.\nPreview  Julia, Python, Java, and C++ are compared for implementing the same iterative algorithm (knapsack solver). The implementation does virtually no memory allocation, so what’s being tested is the speed of looping and array access. Julia is fast (although not quite as fast as C++ or Java). Python, however, comes out looking horribly slow by comparison.   Benchmark problem For a simple yet realistic test of performance let’s code a solution to the 0/1 knapsack problem. I am sure you’ve heard of it – it is a classic discrete optimization problem in its own right and shows up as a building block within many optimization algorithms.\nAs a reminder, the problem is to pack a knapsack of finite weight capacity with a chosen set of items of varying value so as to maximize the total packed value:\n\\[ \\begin{equation*} \\begin{aligned} \u0026amp; \\underset{x}{\\text{maximize}} \u0026amp; \u0026amp; V = \\sum_{i=1}^n v_i x_i \\\\ \u0026amp; \\text{subject to} \u0026amp; \u0026amp; \\sum_{i=1}^n w_i x_i \\leq W \\\\ \u0026amp; \u0026amp; \u0026amp; x_i \\in \\{0,1\\} \\text{ for all } i \\text{ in } \\{1,\\dots,n\\} \\end{aligned} \\end{equation*} \\] We have \\(n\\) items of integral1 weights \\(w_i \u0026gt; 0\\) and value \\(v_i\\). \\(W\\) is the knapsack weight capacity. Each item is either chosen or not, hence the “0/1” in the name.\nSince all \\(x_i\\) are restricted to be binary (boolean), this is an integer linear problem with a search space size of \\(2^n\\). It can be given to an MILP solver. But because of a simple nested structure the problem also has a straightforward dynamic programming (DP) solution that I can code in a few lines.\n Dynamic programming algorithm Assume the problem has been solved for all knapsack capacities up to some \\(w\\) while making use of only a subset of items \\(1, \\dots, j\\), and let \\(V(w, j)\\) be the solution, i.e. the maximum achievable knapsack value. Now consider all smaller subproblems without item \\(j\\), \\(V(w\u0026#39;, j-1)\\) for all possible \\(w\u0026#39;\\). \\(V(w, j)\\) and \\(V(w\u0026#39;, j-1)\\) are connected with a single “move” of looking at item \\(j\\) and deciding to either include it in the optimal selection for knapsack \\(V(w, j)\\) or not:\n if including item \\(j\\) is a move that’s feasible (\\(w_j \\leq w\\)) then \\(V(w, j)\\) is the best of \\(v_j + V(w\u0026#39;, j - 1)\\) and \\(V(w\u0026#39;, j - 1)\\) depending on whether the move is “optimal” (improves \\(V\\)) or not; in the former case \\(w\u0026#39; = w - w_j\\) and in the latter \\(w\u0026#39; = w\\); if including item \\(j\\) is not feasible (\\(w_j \u0026gt; w\\)) then \\(V(w, j) = V(w\u0026#39;, j - 1)\\) and \\(w\u0026#39; = w\\); \\(V(w, 1)\\) is 0 or \\(v_1\\) depending on whether the first item fits within capacity \\(w\\).  In other words, knapsack subproblems have a recurrent relationship for \\(j \u0026gt; 1\\)\n\\[ V(w, j) = \\left\\{ \\begin{array}{@{}ll@{}} V(w , j - 1) \u0026amp; \\text{if}\\ w \\lt w_j, \\\\ \\max \\big\\{v_j + V(w - w_j, j - 1),V(w , j - 1)\\big\\} , \u0026amp; \\text{otherwise} \\\\ \\end{array} \\right. \\] together with a boundary condition for \\(j = 1\\) \\[ V(w, 1) = \\left\\{ \\begin{array}{@{}ll@{}} 0 \u0026amp; \\text{if}\\ w \\lt w_1, \\\\ v_1, \u0026amp; \\text{otherwise} \\\\ \\end{array} \\right. \\]\n(The above could be stated more compactly if \\(j\\) started at 0 and \\(V(w,0)\\) were defined as 0.)\nTo capture inputs into a problem instance, I will use an array of Julia structs. Although I could get away with a list or a tuple, I think a struct makes for more readable example code:\nstruct Item value ::Int64 weight ::Int64 end If \\(V(w, j)\\) is represented as a matrix (another first-class citizen in Julia), a non-recursive implementation would be to sweep the space of \\(w\\) and \\(j\\) going from smaller to larger item sets:\nfunction opt_value(W ::Int64, items ::Array{Item}) ::Int64 n = length(items) # V[w,j] stores opt value achievable with capacity \u0026#39;w\u0026#39; and using items \u0026#39;1..j\u0026#39;: V = Array{Int64}(undef, W, n) # W×n matrix with uninitialized storage # initialize first column v[:, 1] to trivial single-item solutions: V[:, 1] .= 0 V[items[1].weight:end, 1] .= items[1].value # do a pass through remaining columns: for j in 2 : n itemⱼ = items[j] for w in 1 : W V_without_itemⱼ = V[w, j - 1] V_allow_itemⱼ = (w \u0026lt; itemⱼ.weight ? V_without_itemⱼ : (itemⱼ.value + (w ≠ itemⱼ.weight ? V[w - itemⱼ.weight, j - 1] : 0))) V[w, j] = max(V_allow_itemⱼ, V_without_itemⱼ) end end return V[W, n] end That loop is almost a straightforward translation of my recurrence, although the body needs some extra edge condition checks. Note the familiar ?: syntax for ternary operator and some comforting exploitation of Julia’s support for Unicode math symbols.\nThis may not be the best bit of Julia code you’ll ever see, but I’ve literally just learned enough syntax for my first experiment here. One performance-related concession to Julia has already been included above by arranging to traverse \\(V\\) in column-major order. Yes, Julia opts for R/MATLAB/Fortran design choice here and that is not under end user control, as far as I can tell at this point. Right now, it does feel like R and yet awkwardly different from C++, which would be my production choice for this particular problem.\n An obvious improvement Notice also that I cheat a little and do not compute half of the solution, specifically the optimal \\(x_i\\)’s2. This is to keep the experiment simple and focus on iterative performance only. However, since this means I don’t need to capture the optimal solution structure in \\(V\\) matrix, I don’t need to allocate it: it is sufficient to allocate a couple of buffers for columns \\(j\\) and \\(j-1\\) and re-use them as needed:\nfunction opt_value(W ::Int64, items ::Array{Item}) ::Int64 n = length(items) V = zeros(Int64, W) # single column of size W, zero-initialized V_prev = Array{Int64}(undef, W) # single column of size W, uninitialized storage V[items[1].weight:end] .= items[1].value for j in 2 : n V, V_prev = V_prev, V itemⱼ = items[j] for w in 1 : W V_without_itemⱼ = V_prev[w] V_allow_itemⱼ = (w \u0026lt; itemⱼ.weight ? V_without_itemⱼ : (itemⱼ.value + (w ≠ itemⱼ.weight ? V_prev[w - itemⱼ.weight] : 0))) V[w] = max(V_allow_itemⱼ, V_without_itemⱼ) end end return V[W] end This version will now scale to quite large problem instances.\n Algorithm speed: Julia vs Python vs Java vs C++ To measure performance, I use the @elapsed macro from Julia’s family of @time and friends with randomly constructed problem instances of different scale. Because Julia is JIT-based I need to be careful and do a few timing repeats after burning the very first measurement.\nActually, I’ll use the median of 5 repeats which is even more robust:\nfunction run(repeats = 5) @assert repeats \u0026gt; 1 times = zeros(Float64, repeats) seed = 12345 for W in [5_000, 10_000, 20_000, 40_000, 80_000] for repeat in 1 : repeats spec = make_random_data(W, seed += 1) times[repeat] = @elapsed opt_value(spec[1], spec[2]) end sort!(times) println(W, \u0026quot;, \u0026quot;, times[(repeats + 1) ÷ 2]) end end For the promised language shootout, I’ve implemented this same algorithm in three other languages: Python, Java, and C++. I event went as far as make sure that all “ports” used similar array data types and even the exact same random number generator for building the exact same random problem instances. To maintain consistency with statically typed languages (Java, C++), I ensure that value types used by the dynamic versions are 64-bit integers as much as possible – this required some minor contortions in Python.\nHere’s the C++ version of opt_value() (you can get all four language versions from this github repo, there is a single source file per language that’s trivial to compile and run):\nint64_t opt_value (int64_t const W, std::vector\u0026lt;item\u0026gt; const \u0026amp; items) { auto const n = items.size (); std::unique_ptr\u0026lt;int64_t []\u0026gt; const v_data { std::make_unique\u0026lt;int64_t []\u0026gt; (W) }; // \u0026quot;zeros\u0026quot; std::unique_ptr\u0026lt;int64_t []\u0026gt; const v_prev_data { std::make_unique\u0026lt;int64_t []\u0026gt; (W) }; // \u0026quot;zeros\u0026quot; int64_t * V { v_data.get () }; int64_t * V_prev { v_prev_data.get () }; for (int64_t w = items[0].weight; w \u0026lt;= W; ++ w) V[w - 1] = items[0].value; for (std::size_t j = 1; j \u0026lt; n; ++ j) { std::swap (V, V_prev); item const \u0026amp; item { items [j] }; for (int64_t w = 1; w \u0026lt;= W; ++ w) { auto const V_without_item_j = V_prev[w - 1]; auto const V_allow_item_j = (w \u0026lt; item.weight ? V_without_item_j : (item.value + (w != item.weight ? V_prev[w - 1 - item.weight] : 0))); V[w - 1] = std::max(V_allow_item_j, V_without_item_j); } } return V[W - 1]; } I’ve tried to keep the implementation consistent and idiomatic within each language. (It helps that my algorithm is basically a couple of nested loops without memory allocation, some arithmetic, and not much else.) All these implementations could in principle be tuned further using language-specific techniques, but that’s not my goal here.\nHere is a snippet of calculation latency data for \\(W=80000\\), captured on the exact same CPU:\n  lang  W  time (sec)      julia  80000  0.1135    python  80000  28.2094    java  80000  0.0781    c++  80000  0.0710     Hmm. Perhaps you begin to suspect that Python is not going to win this. Here is a log-log plot of all data, for all \\(W\\)’s:\n Figure 1: Calculation time as a function of knapsack problem size. (Note that both axes use log scale.)  Julia is about 50% slower than either Java or C++. But it is Python that is the real laggard in the group: slower by more than 100x across the entire range of tested problem sizes. Ouch!\nGoing back to the matrix version of the algorithm, you can see how the matrix is accessed predictably over \\(j\\) columns but somewhat randomly (in a data-depedent fashion) over \\(w\\) rows. There doesn’t seem to be a way to express the algorithm in linear algebra operations. This difficulty is retained in the optimized two-column-buffer version. The algorithm is short and simple but there just doesn’t seem to be a way to vectorize it so that it would be fast, say, in numpy. Massive underperformance of the Python version is the cost I pay for slow interpretation of Python bytecode.\nAs for Julia, so far so good. I might just get used to the “no need to vectorize code to make it fast” lifestyle.\n  Restricting weights to be integral is not crucial to my test here but could make the problem statement acceptable to more solver types.↩\n Some might argue that \\(x_i\\)’s are in fact more than half of the solution. Ok, so I cheat a lot.↩\n   ","date":1569974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569974400,"objectID":"b8c7a1dfd86e76bae738e1a867b98081","permalink":"/tutorials/study_julia_with_me/knapsack_benchmark/","publishdate":"2019-10-02T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/knapsack_benchmark/","section":"tutorials","summary":"Every once in a while, you need to try an algorithm that’s “custom enough” that it must be coded directly in your “research language”. Perhaps the existing “high-performance” libs don’t support your desired variant or it’s too much setup work switching to another language just to prototype a quick one-off idea. This an instance of the “two language problem” that’s been the reality of technical computing for decades.\nUsers of R, Python, and similar “high-productivity” environments know that native control flow constructs (for-loops, etc) in those languages kill performance unless they can be expressed in “vectorized” form.","tags":null,"title":"How do you say \"0/1 knapsack\" in four languages?","type":"docs"},{"authors":null,"categories":null,"content":" Julia is unusual in that it is a dynamically typed language (meaning you don\u0026rsquo;t have to declare variable types \u0026ldquo;statically\u0026rdquo;, in program text), while at the same time supporting a rich mechanism for communicating type information to the JIT compiler and runtime. Effectively, Julia has both dynamic and static typing. When provided, static type information can have non-trivial impact on Julia code behavior, both in terms of functionality and performance.\nThis distinctive typing seems to be one of the defining features of Julia. To keep tutorials to a reasonable length, I cover basics here and delay further case studies (parameterized types, overloading, multiple dispatch) until the next one.\nPreview  A quick tour of Julia type taxonomy. Drill into type annotations and understand their impact on: code documentation, correctness, performance. Julia functions are always \u0026ldquo;virtual\u0026rdquo;. Julia supports functional OOP as a paradigm.  Types and inheritance Everything in Julia that is a value also has a type and the types are first-class objects (think T.class in Java or T.__type__ in Python). Subtyping relationships are set up and queried with the \u0026lt;: subtype operator:\njulia\u0026gt; typeof(1) Int64 julia\u0026gt; supertype(typeof(1)) Signed julia\u0026gt; typeof(1) \u0026lt;: supertype(typeof(2)) true julia\u0026gt; Int64 \u0026lt;: Signed true julia\u0026gt; Int64 \u0026gt;: Signed # there is also a 'supertype operator', used trivially here but more useful for parameterized types false  Types known to a given runtime session form a tree (a directed graph, actually) rooted at Any.\nSpeaking of [type] trees I found it handy to explore subtrees within this graph of types using the following helper function. It builds on subtypes() from InteractiveUtils.jl, which would be already imported and availalbe if you\u0026rsquo;re in REPL but may need an import in a script:\nimport InteractiveUtils function subtypetree(T, depth = 0) # you might also want to add 'max_depth'... println('\\t' ^ depth, T) for t in InteractiveUtils.subtypes(T) subtypetree(t, depth + 1) end end  Arithmetic types There is a collection of (ahem) typical types for arithmetic:\n Int8, \u0026hellip;, Int64, Int128, plus unsigned variants, Float16, Float32, Float64, BigInt and BigFloat,  organized in this hierarchy:\njulia\u0026gt; subtypetree(Number) Number Complex Real AbstractFloat BigFloat Float16 Float32 Float64 AbstractIrrational Irrational Integer Bool Signed BigInt Int128 Int16 Int32 Int64 Int8 Unsigned UInt128 UInt16 UInt32 UInt64 UInt8 Rational  The first thing that struck me here is Julia\u0026rsquo;s keeping with its emphasis on performance: Julia integers are not \u0026ldquo;big\u0026rdquo; by default (contrast with Python 3). And the rich spectrum of arithmetic bit widths is meaningful: Float16 and Float32 could be useful for GPU computing, while wide Int-types could work with SSE/AVX/etc instructions and/or support efficient interfacing with native code. (I say \u0026ldquo;could\u0026rdquo; because I have no idea yet if that\u0026rsquo;s really the case.)\nPrimitive types Turns out all of these arithmetic types are not what you\u0026rsquo;d call \u0026ldquo;built into the compiler\u0026rdquo; but are rather defined in the language itself, as standard primitive types1:\nprimitive type Bool \u0026lt;: Integer 8 end primitive type Int64 \u0026lt;: Signed 64 end  The syntax is\nprimitive type «name» \u0026lt;: «supertype» «bits» end  where the supertype is optional (and defaults to Any). I can apparently define my own primitive type, a 24-bit integer2:\njulia\u0026gt; primitive type Int24 \u0026lt;: Signed 24 end julia\u0026gt; subtypetree(Number) Number Complex Real ... Integer Bool Signed ... Int24 ...  (This is great\u0026hellip; but how do I construct an Int24 or define arithmetic? Julia docs are not clear on that \u0026ndash; I think I know how to proceed but that is outside of today\u0026rsquo;s scope.)\nConcrete vs abstract types I\u0026rsquo;ve already used a struct in the knapsack benchmark to represent knapsack items as instances of this type:\nstruct Item value ::Int64 weight ::Int64 end  These are straightforward: Items contain fields (the type is composite) and can be instantiated (the type is concrete). They can also be examined at runtime using Julia\u0026rsquo;s reflection facilities:\njulia\u0026gt; sizeof(Item) 16 julia\u0026gt; fieldcount(Item) 2 julia\u0026gt; fieldnames(Item) (:value, :weight) julia\u0026gt; fieldtypes(Item) (Int64, Int64) julia\u0026gt; function showfields(T) for i in 1 : fieldcount(T) println(fieldoffset(T, i), '\\t', fieldname(T, i), \u0026quot;\\t::\u0026quot;, fieldtype(T, i)) end end showfields (generic function with 1 method) julia\u0026gt; showfields(Item) 0 value ::Int64 8 weight ::Int64  but a few things here are less obvious:\n structs default to being immutable unless explicitly marked as mutable. they are always final, i.e. cannot be further inherited from3. (This is also true of primitive and, in fact, any non-abstract types.)  Defaulting to immutability is not an arbitrary choice. It can be exploited by the compiler to optimize performance and memory usage by \u0026ldquo;interning\u0026rdquo; values that are indistinguishable if they are equal. Compare\njulia\u0026gt; i1 = Item(1, 2) Item(1, 2) julia\u0026gt; i2 = Item(1, 2) Item(1, 2) julia\u0026gt; i1 == i2 # equal true julia\u0026gt; i1 === i2 # actually, the same \u0026quot;interned\u0026quot; object true julia\u0026gt; i1 == Item(1, 3) false  with\njulia\u0026gt; mutable struct MutableItem # mutable version of 'Item' value ::Int64 weight ::Int64 end julia\u0026gt; i1 = MutableItem(1, 2) MutableItem(1, 2) julia\u0026gt; i2 = MutableItem(1, 2) MutableItem(1, 2) julia\u0026gt; i1 == i2 # not equal! false # \u0026lt;- surprised? looks like '==' needs to be defined for custom mutable types...  Julia also has abstract types which can\u0026rsquo;t be instantiated, but are instead used to organize the type graph via shared parent nodes. You could also say they act as \u0026ldquo;marker\u0026rdquo; or \u0026ldquo;trait\u0026rdquo; base classes, like Number or Signed above. Since all non-abstract Julia types are final, any supertype is necessarily an abstract type (Any if not specified explicitly).\njulia\u0026gt; abstract type MyInt \u0026lt;: Int32 end ERROR: invalid subtyping in definition of MyInt Stacktrace: [1] ... julia\u0026gt; abstract type MyInt \u0026lt;: supertype(Int32) end julia\u0026gt; subtypetree(Number) Number Complex Real ... Integer Bool Signed ... Int24 ... MyInt ...  Type parameters To various degrees, all of the three major categories of Julia types (primitive, composite, abstract) are available in other dynamic languages, either natively or via some libraries. Julia also offers something that gets it if not into the realm of uber-powerful (and uber-complicated) C++ metaprogramming, then definitely into the realm of Java generics: all three type categories can be further parameterized with other types and values. I\u0026rsquo;ll explore this in a future tutorial.\nType annotations And now to the meat of this tutorial: type annotations. A type annotation in Julia looks like \u0026lt;thing\u0026gt;::\u0026lt;type\u0026gt;4 \u0026ndash; it is an in-place modifier to a \u0026lt;thing\u0026gt; introduced by the :: operator.\nTypeasserts vs variable declarations The way I read the documentation, Julia type annotations can be applied to two types of \u0026lt;thing\u0026gt;s:\n [typeassert] expressions computing a value (and recall that everything in Julia is an expression):\nx = y ::Float64 # promises to the runtime that at this point in the execution 'y' will be a Float64  [variable type declaration] left-hand sides of assignments or declarations that introduce (local) variables:\nx ::Float64 = y # declares a new local 'x', marks it as always containing Float64 values, and initializes with 'y' converted to Float64  This second case also covers typed fields of structs and named tuples:\nstruct Point x ::Float64 # this field will always contain only Float64 values y # this field can contain any Julia value end   The first case is a typeassert. The second kind of annotation marks the name/field to its left as constrained to values compatible with the given type, and also ensures that throughout the variable\u0026rsquo;s scope all subsequent initializations of and assignments to it are filtered through an implicit conversion.\nAs a consequence, there are differences in runtime behavior and the information communicated to the system:\n With a typeassert the compiler will create code that at runtime will check the annotated value for type compatibility and throw a TypeError if the check fails \u0026ndash; but it will not attempt to coerce the computed value to the annotation type in any way. Type-asserted syntax \u0026lt;exp\u0026gt;::T is precisely equivalent to a call, possibly inlined, to typeassert(\u0026lt;exp\u0026gt;, T) followed by making use of the \u0026lt;exp\u0026gt; value. With a variable type declaration any assignment \u0026lt;lhs\u0026gt;::T\u0026nbsp;=\u0026nbsp;\u0026lt;rhs\u0026gt; will effectively translate into \u0026lt;lhs\u0026gt;\u0026nbsp;=\u0026nbsp;convert(T, \u0026lt;rhs\u0026gt;), i.e. contain a call, possibly inlined, to convert(T, \u0026lt;rhs\u0026gt;). And at runtime, every such assignment will attempt to coerce its right hand-side value to T, possibly resulting in a value that\u0026rsquo;s only an approximation. Should this conversion fail, an exception will be thrown:  if no such conversion exists at all, a MethodError is thrown; if T is an Integer (sub)type and cannot represent the expression value, an InexactError is thrown.   To appreciate the difference, compare\njulia\u0026gt; function foo() x ::Float64 = 1 # implies 'convert(Float64, 1)' x, typeof(x) end foo (generic function with 1 method) julia\u0026gt; foo() (1.0, Float64)  with\njulia\u0026gt; function foo() x = 1 ::Float64 # implies 'typeassert(1, Float64)' x, typeof(x) end foo (generic function with 1 method) julia\u0026gt; foo() ERROR: TypeError: in typeassert, expected Float64, got Int64  Literal 1 is of a (machine-dependent) Int type:\njulia\u0026gt; typeof(1) Int64  and even though it can be converted to a Float64 without loss, such a conversion is not even attempted in the second version of foo().\n As I write this, Julia does not yet support type declarations for global variables \u0026ndash; this is the reason I wrapped the above examples into functions.   What about function signatures? Unsurprisingly, it is also possible to type-annotate function arguments and return types:\nfunction bar(x ::Float64) ::Float32 sin(2π * x) end  Function arguments If you\u0026rsquo;re coming from languages like C++ or Java where type conversions can happen as part of argument passing, you might think that x ::Float64 in bar() is like a local (typed) variable declaration, similar to the second case above, perhaps implying a call to something like convert(Float64, x) everywhere before bar() is invoked. That is not the case in Julia: no conversions ever take place as part of Julia function argument passing. In fact, Julia argument type annotations are actually more like those typeasserts: foo(x) will expect x to be a Float64 already.\nThere is a subtle difference from an in-place typeassert, however: with the above definition of bar() there will be no need to generate an implicit call to typeassert() at all because I will only be allowed to call it with Float64s. If I need sin(2π\u0026nbsp;*\u0026nbsp;x) for a Float64 input x, no problem. For any other type5, say, Int64, I will get a flat rejection not because a method call was tried and failed during Int64-to-Float64 input type conversion (TypeError) but because the requisite method (named \u0026ldquo;foo\u0026rdquo; and taking a single argument of type Int64) did not exist (MethodError). And since Float64 is a concrete type and, again, all concrete types are final in Julia, the universe of possible outcomes here shrinks dramatically:\njulia\u0026gt; bar(0.75) -1.0f0 julia\u0026gt; bar(1.) -2.4492937f-16 julia\u0026gt; bar(1) ERROR: MethodError: no method matching bar(::Int64) Closest candidates are: bar(::Float64) at ...  This may seem a little draconian, but it is connected to how Julia\u0026rsquo;s multiple dispatch works and is further ameliorated by Julia\u0026rsquo;s system of promoting function arguments to a common type.\nFunction return types Specifying bar() return type to be Float32 is a way to ensure that value being returned is passed through a convert(Float32, \u0026hellip;). Whether this is desired depends on software design. I can imagine situations where it could be used as a way to safely return \u0026ldquo;special\u0026rdquo; values:\nfunction sqrt_or_nothing(x ::Float64) ::Union{Float64, Nothing} x \u0026lt; 0.0 ? nothing : √x end  julia\u0026gt; @show sqrt_or_nothing(2.0) sqrt_or_nothing(2.0) = 1.4142135623730951 1.414213562373095 julia\u0026gt; @show sqrt_or_nothing(-2.0) sqrt_or_nothing(-2.0) = nothing  Alternatively, it might be easier to reason about your code behavior if most functions are strict about their return value types. Otherwise, it seems like it could be easy in Julia to accidentally return different types along different value return paths, which could cause inefficiencies or maybe even errors downstream:\nfunction bar_clipped(x ::Float64) x \u0026lt; 0.0 ? 0 : sin(2π * x) end  julia\u0026gt; typeof(bar_clipped(0.75)) Float64 julia\u0026gt; typeof(bar_clipped(-0.75)) Int64 # oops, use 0.0 literal instead of 0 above  When are implicit conversions done? Most of the cases of implicit calls to convert(\u0026hellip;) have already been mentioned. Julia documentation offers this complete list:\n  Assigning to an array converts to the array\u0026rsquo;s element type. Assigning to a field of an object converts to the declared type of the field. Constructing an object with new converts to the object\u0026rsquo;s declared field types. Assigning to a variable with a declared type (e.g. local x::T) converts to that type. A function with a declared return type converts its return value to that type. Passing a value to ccall converts it to the corresponding argument type.   Case study: functional OOP So far, Julia type annotations appeared to be potentially beneficial (for code maintenance, performance), yet somehow optional, feature of the language. Let me now show a situation where annotation are truly necessary.\nWe saw how every function argument in Julia is always associated with a type. Is it possible for there to be multiple functions that all have the same name but different parameter types?\nNot only is the answer \u0026ldquo;yes\u0026rdquo;, it is actually kind of like \u0026ldquo;yes, it is meant to happen a lot\u0026ldquo;: Julia thrives on maintaining multiple versions of the \u0026ldquo;same\u0026rdquo; function (called \u0026ldquo;methods\u0026rdquo;) and figuring out which version to invoke for a given set of inputs. These versions are distinguished by annotating parameters with different types. Enter \u0026ldquo;multiple dispatch\u0026rdquo;, a core paradigm of Julia programming6. The intuition is that Julia functions are essentially \u0026ldquo;always virtual\u0026rdquo;: unlike other languages where a class method needs to be marked in a special way to support \u0026ldquo;late binding\u0026rdquo; (method dispatch based on runtime, not compile, type of an object), Julia runtime system always dispatches all functions on the concrete runtime types of all their arguments. Because no argument position is \u0026ldquo;special\u0026rdquo; and the method does not \u0026ldquo;belong\u0026rdquo; to any particular parameter type, this form of polymorphism usually opts for language design with standalone functions, that is functions that do not live inside any \u0026ldquo;classes\u0026rdquo;. Some people call such designs \u0026ldquo;functional OOP\u0026rdquo;. It makes a lot of sense for math-style coding due to symmetries in function parameters.\nNow, one way to ease into Julia multiple dispatch is to consider its simplest edge case: single dispatch.\nSerializing Julia objects to JSON I am going to implement a simple serializer of Julia objects to JSON. This will be a toy example, useless in any kind of production setting. My initial point is the \u0026ldquo;interface\u0026rdquo; method to_JSON() calling visit() that in turn starts as a single fallback that always fails:\nfunction to_JSON(io ::IO, obj) visit(obj, io) end function visit(obj, io ::IO) error(\u0026quot;default visit() called for obj type: \u0026quot;, typeof(obj)) end  I am going to anchor my design in the \u0026ldquo;virtual nature\u0026rdquo; of visit(obj,\u0026nbsp;io), with execution routed based on the runtime type of obj. Looking at my Number type trees above, I can see that this overload can cover JSON numbers and booleans:\nfunction visit(obj ::Real, io ::IO) print(io, obj) end  This overload will kick in for any obj that belongs to a type derived from Real \u0026ndash; because that is more specific than Any and because Julia dispatch algorithm will always choose the most specific method signature to call in every situation.\nStrings are equally easy, but the method body needs to quote them7, so I need a new method overload:\nfunction visit(obj ::AbstractString, io ::IO) print(io, '\u0026quot;') print(io, obj) print(io, '\u0026quot;') end  So far, I have told Julia to dispatch execution to either Real (and its subtypes, which include ints, floats, and booleans) or AbstractString (and its subtypes, including String and everything that is string-like).\n By the way, the actual dispatch decision is based on (obj,\u0026nbsp;io) but io happens to be the same across all visit()s, so the dispatch is effectively on a single argument obj.\nWhy do I suggest thinking of visit() as \u0026ldquo;virtual\u0026rdquo;? Think of visit(obj,\u0026hellip;) as equivalent to obj.visit(\u0026hellip;) in a language like Python, Java, C++, where the version of visit() to use depends on the runtime type of obj.\n  Also note how abstract parent types come in handy: I don\u0026rsquo;t need to code explicit visit(Float32,\u0026hellip;), visit(Float64,\u0026hellip;), visit(BigFloat,\u0026hellip;) for all possible (and future!) leaves of the type tree because I can handle things at the level of abstract parent nodes.\nBy this point, my imlementation roadmap should be apparent: I am going to keep adding more visit() overloads with obj parameter types chosen so as to partition the type universe into subtrees that correctly \u0026ldquo;carve out\u0026rdquo; each supported type of anything I expect to find inside my input. Taking the next step, for objects that can contain other objects the virtual nature of visit() becomes critical:\nfunction visit(obj ::AbstractArray, io ::IO) print(io, '[') for i in 1 : length(obj) i \u0026gt; 1 \u0026amp;\u0026amp; print(io, \u0026quot;, \u0026quot;) visit(obj[i], io) end print(io, ']') end function visit(obj ::AbstractDict, io ::IO) print(io, '{') first = true for (k, v) in obj first ? first = false : print(io, \u0026quot;, \u0026quot;) visit(k ::AbstractString, io) # assert that key is a string print(io, \u0026quot; : \u0026quot;) visit(v, io) end print(io, '}') end  The nested visit()s are already virtual, nothing else needs to be done to pick up a particular overload. There are also no \u0026ldquo;if-obj-type-is-\u0026hellip;\u0026rdquo; condition checks \u0026ndash; everything is as clean as in \u0026ldquo;pure\u0026rdquo; textbook OOP.\nJust a handful of lines of code so far, and yet they already work on a variety of inputs:\njulia\u0026gt; to_JSON(stdout, [1, false, zeros(2), [1.2345, 12, Dict(\u0026quot;a\u0026quot; =\u0026gt; true, \u0026quot;b\u0026quot; =\u0026gt; 2.3, \u0026quot;c\u0026quot; =\u0026gt; [1, 2, 3.4])]]) [1, false, [0.0, 0.0], [1.2345, 12, {\u0026quot;c\u0026quot; : [1.0, 2.0, 3.4], \u0026quot;b\u0026quot; : 2.3, \u0026quot;a\u0026quot; : true}]]  Not bad, looks like valid JSON to me.\nNow, suppose that I would like to extend the set of supported \u0026ldquo;JSON-compatible\u0026rdquo; Julia types to also include tuples. I would like to output them as JSON arrays. All I need to do is add another overload:\nfunction visit(obj ::Tuple, io ::IO) print(io, '[') for i in 1 : length(obj) i \u0026gt; 1 \u0026amp;\u0026amp; print(io, \u0026quot;, \u0026quot;) visit(obj[i], io) end print(io, ']') end  julia\u0026gt; to_JSON(stdout, [(\u0026quot;tuple\u0026quot;, (1, \u0026quot;nested\u0026quot;, \u0026quot;tuple\u0026quot;)) 1, false, zeros(2), [1.2345, 12, Dict(\u0026quot;a\u0026quot; =\u0026gt; true, \u0026quot;b\u0026quot; =\u0026gt; 2.3, \u0026quot;c\u0026quot; =\u0026gt; [1, 2, 3.4])]]) [[\u0026quot;tuple\u0026quot;, [1, \u0026quot;nested\u0026quot;, \u0026quot;tuple\u0026quot;]], 1, false, [0.0, 0.0], [1.2345, 12, {\u0026quot;c\u0026quot; : [1.0, 2.0, 3.4], \u0026quot;b\u0026quot; : 2.3, \u0026quot;a\u0026quot; : true}]]   Observe how no earlier visit()s needed to be modified to become \u0026ldquo;aware\u0026rdquo; of this new support for tuples. In other words, a method added later hooks into a set of mutual method invocations coded earlier, with no ostensible \u0026ldquo;recompilation\u0026rdquo;.   If you\u0026rsquo;re interested in playing with various design alternatives yourself, you can find this entire example here.\nSummary In summary, I would like to call out some things about Julia typing that make Julia feel different from many languages:\n Julia supports user-definable primitive types which do not need to be \u0026ldquo;boxed\u0026rdquo;. All superclasses are abstract and all concrete classes are final. Mutability is part of type definition, not of argument/variable/field. Multiple dispatch is a primary paradigm (\u0026ldquo;all methods are virtual\u0026rdquo;). This design is consistent with lack of classic \u0026ldquo;objects\u0026rdquo; that forcefully bundle state with behavior. Duck-typing also works in Julia, perhaps even without any performance loss. But certain core Julia features necessitate some static typing.   This is what the public documentation says. Examining boot.jl of my Julia install shows that many of these types are actually implemented in C. ^ As I write this, bit widths must be multiples of 8. ^ Just like classes marked with final keyword in C++ or Java. ^ I use an extra blank before :: but you don\u0026rsquo;t have to. ^ To simply the narrative, I am glossing over possibilities for promotion of x to another type. ^ From what I can tell so far, that is. ^ Note that this toy serializer doesn\u0026rsquo;t bother with backslash escapes, Unicode, etc. ^   ","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"50ff1c895ff1937cdfe929286f925115","permalink":"/tutorials/study_julia_with_me/type_annotations/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/type_annotations/","section":"tutorials","summary":"Julia is unusual in that it is a dynamically typed language (meaning you don\u0026rsquo;t have to declare variable types \u0026ldquo;statically\u0026rdquo;, in program text), while at the same time supporting a rich mechanism for communicating type information to the JIT compiler and runtime. Effectively, Julia has both dynamic and static typing. When provided, static type information can have non-trivial impact on Julia code behavior, both in terms of functionality and performance.","tags":null,"title":"Julia type annotations: what Python 'type hints' wish they were","type":"docs"},{"authors":null,"categories":null,"content":" I mentioned Julia’s fondness for multiple dispatch in my first typing tutorial. On purpose at the time, my case study was limited to the simplest case: single dispatch. I’ve waited until I ran across an example of a “natural”\" problem where multiple dispatch would play an important role. I think I’ve found one: differentiating a function with no closed-form formula.\nPreview  Play with a custom number-like type. Simplify implementation by leveraging Julia’s support for type promotions. Review (very quickly) one technique for automated differentiation (AD). Implement Forward AD through operator overloading. Test it in several ways, finish by plugging it into a Newton-Raphson solver.   Differentiate this To set my overall objective, I would like to be able to compute the derivative of the following function:\nfunction myerf(x ::Number) Σ = 0.0 x² = x * x for k in 0 : 20 # hardcoding the number of summation terms for simplicity Σ += x / (factorial(k) * (2k + 1)) x *= -x² end return 2.0 / √π * Σ end This just sums a truncated Taylor series for erf(x): \\[ \\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{-\\infty}^{x}e^{-t^2}dt = \\frac{2}{\\sqrt{\\pi}} \\sum_{k=0}^{\\infty} \\frac{(-1)^k x^{2k+1}}{k! (2k+1)} \\] This series is straightforward to derive by integrating the one for \\(e^{-x^2}\\). That fact will also allow me to easily check my derivative function, when I get one, against the known correct answer1: \\[ \\frac{\\partial}{\\partial x} \\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} e^{-x^2} \\] My sum-of-the-series implementation seems correct2:\n# if you haven\u0026#39;t installed SpecialFunctions yet: # pkg\u0026gt; add SpecialFunctions julia\u0026gt;using SpecialFunctions # for the \u0026quot;official\u0026quot; erf(x) julia\u0026gt; myerf(0.7), erf(0.7) (0.6778011938374184, 0.6778011938374184) So, I know the closed-form expression for the derivative of myerf(x) and somehow I need to get the computer to get it, too, but without actually entering the closed-form answer. And, by the way, I would like a solution that’s as exact as possible, something not based on finite differences.\n Baby Julia steps: operators, functions, and methods To understand the AD example that’s coming up, it will be helpful to delve a little bit into Julia’s model of function dispatch and evaluation. It may seem like an unnecessarily long detour but I promise it will all come together and the learnings will be general.\nFor the following one-line function definition\njulia\u0026gt; g(x) = x * (x + x) g (generic function with 1 method) consider how the inputs (of which \\(g\\) has only one, \\(x\\)) determine its eventual value3: we sum two copies of \\(x\\), then multiply that intermediate result by another \\(x\\). You could say that all these values, both the inputs and the intermediates, propagate through \\(g\\)’s calculation graph:\nEach function call node in such a graph is an elementary step of the calculation. In fact, I can drill into these steps directly from Julia:\njulia\u0026gt; g_tree = :(x * (x + x)) # \u0026#39;:(...)\u0026#39; is the \u0026quot;quoting\u0026quot; operator here: it parses what\u0026#39;s inside into a syntax tree :(x * (x + x)) julia\u0026gt; typeof(g_tree) Expr julia\u0026gt; dump(g_tree) # show g_tree as a tree of annotated nodes Expr head: Symbol call args: Array{Any}((3,)) 1: Symbol * 2: Symbol x 3: Expr head: Symbol call args: Array{Any}((3,)) 1: Symbol + 2: Symbol x 3: Symbol x What dump() shows is a tree of expression (Expr) nodes comprising the calculation graph for \\(g(x)\\). Each node annotated with Symbol call is an invocation of one elementary step like multiplication (*), addition (+), and so on – those were the diamond-shaped nodes in the picture. (If you have some CS background or ever worked on an interpreter or compiler, this ground should feel very familiar.)\nJulia operators are functions As already implied by the syntax tree form of \\(g(x)\\), almost4 all elementary operators like *, +, etc are actually Julia functions with special infix syntax. They are defined as such and can be invoked with the “normal” function call syntax:\njulia\u0026gt; x = 2; julia\u0026gt; x + x 4 julia\u0026gt; +(x, x) # call function named \u0026quot;+\u0026quot; with parameter tuple \u0026quot;(x, x)\u0026quot; 4 (In fact, most of Julia is defined via its own syntax, something the language designers and user community are justifiably proud of.)\n Julia functions are “overloadable” (or is it “overridable”? 😄) Multiplying two integers is not the same as “multiplying” two strings5:\njulia\u0026gt; x * x 4 julia\u0026gt; \u0026quot;x\u0026quot; * \u0026quot;x\u0026quot; \u0026quot;xx\u0026quot; and might be different from “multiplying” objects of a user-defined type. As I started to explain in my previous typing tutorial, this is made possible by using different parameter types for different function versions (“methods”, in Julia parlance). Internally, Julia keeps a list of all methods it is aware of for each operator/function and they can be examined using methods():\njulia\u0026gt; methods(*) # 358 methods for generic function \u0026quot;*\u0026quot;: [1] *(x::Bool, z::Complex{Bool}) in Base at complex.jl:282 [2] *(x::Bool, y::Bool) in Base at bool.jl:98 [3] *(x::Bool, y::T) where T\u0026lt;:AbstractFloat in Base at bool.jl:110 [4] *(x::Bool, z::Complex) in Base at complex.jl:289 [5] *(x::Bool, y::AbstractIrrational) in Base at irrationals.jl:139 [6] *(a::Float16, b::Float16) in Base at float.jl:392 [7] *(x::Float32, y::Float32) in Base at float.jl:398 [8] *(x::Float64, y::Float64) in Base at float.jl:399 [9] *(z::Complex{Bool}, x::Bool) in Base at complex.jl:283 ... lots more ... It is possible for a user to add more methods to such lists. Each row has a unique signature. Imagine defining your own “numbers”:\njulia\u0026gt; struct MyNum \u0026lt;: Number v ::Float64 end julia\u0026gt; MyNum(2.0) * MyNum(2.0) # this won\u0026#39;t work ERROR: MethodError: no method matching *(::MyNum, ::MyNum) Closest candidates are: *(::Any, ::Any, ::Any, ::Any...) at operators.jl:529 julia\u0026gt; Base.:(*)(lhs ::MyNum, rhs ::MyNum) = MyNum(lhs.v * rhs.v) # define \u0026#39;MyNum * MyNum\u0026#39;, so now it will julia\u0026gt; MyNum(2.0) * MyNum(2.0) MyNum(4.0) julia\u0026gt; methods(MyNum) # 2 methods for generic function \u0026quot;(::Type)\u0026quot;: [1] MyNum(v::Float64) in Main at none:1 [2] MyNum(v) in Main at none:1 julia\u0026gt; methods(*) # 359 methods for generic function \u0026quot;*\u0026quot;: # \u0026lt;- note that count is up by one [1] *(x::Bool, z::Complex{Bool}) in Base at complex.jl:282 ... [31] *(lhs::MyNum, rhs::MyNum) in Main at none:1 ... Note again how the new method doesn’t “belong” to MyNum but rather to a global method table associated with *.\nContinuing, I would need to define a minimal collection of arithmetic operations in order for `MyNum’s to be useful as something resembling numbers:\n# binary ops: Base.:(+)(lhs ::MyNum, rhs ::MyNum) = MyNum(lhs.v + rhs.v) Base.:(-)(lhs ::MyNum, rhs ::MyNum) = MyNum(lhs.v - rhs.v) Base.:(*)(lhs ::MyNum, rhs ::MyNum) = MyNum(lhs.v * rhs.v) Base.:(/)(lhs ::MyNum, rhs ::MyNum) = MyNum(lhs.v / rhs.v) # unary ops: Base.:(+)(x ::MyNum) = x Base.:(-)(x ::MyNum) = MyNum(- x.v) julia\u0026gt; MyNum(3.0) + MyNum(4.0) MyNum(7.0) So far, so good. Note that \\(g(x)\\) is generic enough so it “just works” for both plain floats and MyNums:\njulia\u0026gt; g(2.0) 8.0 julia\u0026gt; g(MyNum(2.0)) MyNum(8.0) Take note, this will happen again later. Of course, \\(g(x)\\) is downright trivial: it uses only * and + – this was truly a baby step.\n Something is missing My custom, soon-to-win-many-awards, number type is missing some boilerplate stuff. Trying another simple function fails; it flat out refuses to work with MyNums:\njulia\u0026gt; f(x) = 1.0 / x f (generic function with 1 method) julia\u0026gt; f(MyNum(2.0)) ERROR: promotion of types Float64 and MyNum failed to change any arguments Stacktrace: [1] sametype_error(::Tuple{Float64,MyNum}) at ./promotion.jl:308 [2] not_sametype(::Tuple{Float64,MyNum}, ::Tuple{Float64,MyNum}) at ./promotion.jl:302 [3] promote at ./promotion.jl:285 [inlined] [4] /(::Float64, ::MyNum) at ./promotion.jl:316 [5] f(::MyNum) at ./none:1 I can see from the stacktrace that \\(f(x)\\) was indeed found and started, but something went wrong later: there was no suitable defition of / for parameter type pair (Float64, MyNum). Julia does not know to convert the 1.0 in 1.0 / x to a MyNum – the language does not have what’s called automatic promotion. As a result, MyNums don’t interoperate with the “stock” numbers particularly well.\nPromotion fix #1: MOAR dispatch! One way to proceed is to keep adding to the method table for / (and other ops) until all conceivable cases are covered:\njulia\u0026gt; Base.:(/)(lhs ::Float64, rhs ::MyNum) = MyNum(lhs / rhs.v) # float / MyNum julia\u0026gt; f(MyNum(2.0)) # f() works now MyNum(0.5) julia\u0026gt; h(x) = x / 3.0 h (generic function with 1 method) julia\u0026gt; h(MyNum(2.0)) ERROR: promotion of types MyNum and Float64 failed to change any arguments ... julia\u0026gt; Base.:(/)(lhs ::MyNum, rhs ::Float64) = MyNum(lhs.v / rhs) # MyNum / float julia\u0026gt; h(MyNum(2.0)) # h() works now, too MyNum(0.6666666666666666) In other words, all possible permutations of parameter types need to be implemented by all operators. This game of whack-a-mole seems laborious and error-prone. Hmm, is it possible to have too much dispatch? 😄\n Again, the “multiple” in Julia method dispatch is apparent if you review all variants of MyNum division so far:\nBase.:(/)(lhs ::MyNum, rhs ::MyNum) = MyNum(lhs.v / rhs.v) Base.:(/)(lhs ::Float64, rhs ::MyNum) = MyNum(lhs / rhs.v) Base.:(/)(lhs ::MyNum, rhs ::Float64) = MyNum(lhs.v / rhs) There is complete symmetry between the left-hand side (lhs) and the right-hand side (rhs) arguments: neither one has “ownership” of /. However, to make all these different overloads co-exist and do slightly different things it is necessary to dispatch on the runtime type of both arguments.\n  This design choice begs for some automatic code generation and indeed I think it can be made viable through Julia’s metaprogramming facilities, specifically macros. But because I haven’t covered metaprogramming yet, I will instead go another, easier for the time being, route.\n Promotion fix #2: promote_rule() Julia has provisions for dealing with the explosion in the number of needed permutations of argument types. The idea is as follows: instead of expecting for there to be an op(T1, T2) method for every possible (T1, T2) pair, a collection of “promotion rules” is queried to find out whether T1 and T2 can be first converted to a common type T (which could be, but doesn’t have to be, either T1 or T2). If that is possible, it is assumed that the conversion is lossless and subsequently only the op(T, T) overload will be needed.\nIt is easy to see how this system reduces the need for very large method tables. In the case of MyNum, the correct incantation is:\nBase.promote_rule(::Type{MyNum}, ::Type{\u0026lt;: Number}) = MyNum which specifies that whenever a mix of MyNum and subtype-of-Number is encountered, the common type T to use is MyNum. This is less “automatic”, but it works. This way Julia supports type promotions that in other languages like C++ and Java are done by the compiler (and hence are fairly fixed).\nSo now a much more compact and yet robust arithmetic system for a custom number type can be implemented with two types of building blocks:\nactual arithmetic/math operators and functions, defined to take only the custom number arguments; if needed, promotion rules to make the custom types interoperate with the “stock” types.  This is the design I use below as part of my demo AD implementation. You can read the handful of lines needed to have custom arithmetic operations in the supporting module file.\nWhew! 😰 It took a bit longer than I thought, but the language mechanics have been explained and I can return to my original objective: fun with differentiating stuff in Julia.\n   Case study: automatic differentiation via “forward prop” What I need now is some Automatic Differentiation (AD). There are many AD flavors but I will exlore just one simple variant called “forward” (also “tangent” or “standard”) AD. Forward AD is particularly easy to engineer in a language that offers operator overloading, which we now understand Julia does. (By the way, if you search the web chances are you will run into tutorials that introduce Forward AD via dual numbers – I do not do that here because I don’t think it is particularly intuition-inducing.)\nYou may have noted that myerf(x) was coded in a profoundly “non-functional” style: it has evil procedural things like variable mutation in a for-loop. Nevertheless, AD can work with it6 and I believe that to be a simple example of differentiable programming.\n Core AD ideas:\nA calculation is a composition of smaller steps These steps can be instrumented to generate “stuff” in addition to the original expression values.     Core AD idea #1: composition of steps If you’ve made it through the preceeding sections, the following should now be seared into your subconscious: a function computes its output by forward-propagating inputs and intermediate expression values through its calculation graph. These elementary steps are +, *, cos, sin, etc.\nFuthermore, these steps are not very “intrinsic” or somehow untouchable: I’ve shown that I could wrap values into a custom type and overload all the operations I am interested in. This works beacuse of Julia’s core paradigm of (multiple) dispatch on argument types.\n Core AD idea #2: step instrumentation Just one more step remaining. Why did I bother replicating standard arithmetic with MyNum? Sure, it was to understand better how some aspects of Julia worked. But note this: computing the derivative of a function is also a composable calculation and it can be done in parallel with computing the function value.\nSo… if I extend MyNum with another field to hold the current expression’s derivative, I can forward-propagate it in the same pass as the main\" value:\nstruct Context \u0026lt;: Number v ::Number ∂ ::Number end # binary ops: Base.:(+)(lhs ::Context, rhs ::Context) = Context(lhs.v + rhs.v, lhs.∂ + rhs.∂) Base.:(-)(lhs ::Context, rhs ::Context) = Context(lhs.v - rhs.v, lhs.∂ - rhs.∂) Base.:(*)(lhs ::Context, rhs ::Context) = Context(lhs.v * rhs.v, lhs.v * rhs.∂ + lhs.∂ * rhs.v) Base.:(/)(lhs ::Context, rhs ::Context) = Context(lhs.v / rhs.v, (lhs.∂ * rhs.v - lhs.v * rhs.∂) / rhs.v^2) # unary ops: Base.:(+)(x ::Context) = x Base.:(-)(x ::Context) = Context(- x.v, - x.∂) Here Context is the grown-up version of MyNum. It has been extended with field \\(\\partial\\). The arithmetic overloads for Context continue to propagate expression values in \\(v\\) while some new logic propagates derivative values in \\(\\partial\\).\nIndividual Context \\(v\\)-rules are composed to form a larger calculation such as myerf(x). Consistently with such pattern of composition, each \\(\\partial\\)-rule can be seen to be an application of the chain rule for obtaining derivatives of the composition of functions. For example, since \\[ \\frac{\\partial}{\\partial x} (lhs + rhs)(x) = \\frac{\\partial}{\\partial x} lhs(x) + \\frac{\\partial}{\\partial x} rhs(x) \\] the Context rule for + must be \\[ +\\big( \\langle lhs.v, lhs.\\partial \\rangle, \\langle rhs.v, rhs.\\partial \\rangle \\big) = \\big\\langle lhs.v + rhs.v, lhs.\\partial + rhs.\\partial \\big\\rangle. \\] The rule for sin, should it be needed, would be \\[ \\sin \\big(\\langle x.v, x.\\partial \\rangle \\big) = \\big\\langle \\sin(x.v), \\cos(x.v) \\partial \\big\\rangle, \\] and so on.\nWhat Context value should be fed as the initial input? Its \\(v\\)-field is x. The derivative of that with respect to x is 1.0, so to establish derivative propagation correctly it needs to be seeded with Context(x, 1.0).\nAnd that’s pretty much it. Does it work? Let’s give it a try:\nfunction derivative(f ::Function) return x ::Number -\u0026gt; f(Context(x, 1.0)).∂ # discard value, return derivative end ∂ = derivative # add a nice-looking alias And the moment of truth:\njulia\u0026gt; include(\u0026quot;multiple_dispatch/FAD.jl\u0026quot;) Main.FAD julia\u0026gt; using Main.FAD # bring ∂, etc into this REPL julia\u0026gt; x = 0.7; julia\u0026gt; ∂(myerf)(x), 2/√π * exp(-x^2) (0.6912748604105389, 0.6912748604105386) Success!\n Observe something neat: myerf(x) hasn’t stopped working for plain float x’s – it is still generic. Invoked with a scalar float x, it will return just the erf(x) value and spend no CPU cycles computing anything else. Invoked with a Context, it will return both erf(x) and its derivative at x. Of course, two different bits of native assembly execute in these two cases. Both versions (that is, methods of a function named “myerf”) will be JIT’ed separately.\nmyerf(Context) is in fact a version of myerf(x) instrumented to carry out additional calculations and carry additional, well, context through the calculation graph that defines myerf(x).\n  As a different test, the chain rule \\(\\frac{\\partial}{\\partial x}(f \\circ g)(x) = \\frac{\\partial}{\\partial g} f(g(x)) \\frac{\\partial}{\\partial x} g(x)\\) should also hold for arbitrary “differentiable” Julia functions:\njulia\u0026gt; f(x) = 1.0 / x f (generic function with 1 method) julia\u0026gt; g(x) = x * (x + x) g (generic function with 1 method) julia\u0026gt; x = 1.2345; julia\u0026gt; ∂(f ∘ g)(x), ∂(f)(g(x)) * ∂(g)(x) (-0.5315286974115385, -0.5315286974115385) I think this is downright beautiful.\nAnd last but not least, let me try an algorithm that can benefit from having both \\(f(x)\\) and \\(\\frac{\\partial}{\\partial x}f(x)\\) available at the same time, e.g. Newton-Raphons root finder. Here is a prototype version performing \\(x_{n+1} = x_n - \\frac{f(x_n)}{\\frac{\\partial}{\\partial x}f(x_n)}\\) iterations until convergence within given tolerance:\nfunction root_solve(f ::Function, x₀ ::Number; ϵ = 1e-8) i = 1 while true ctx = f(Context(x₀, 1.0)) println(\u0026quot;[$i]: f($x₀)\\t= $(ctx.v)\u0026quot;) abs(ctx.v) \u0026lt; ϵ \u0026amp;\u0026amp; break x₀ -= ctx.v / ctx.∂ # use both value and derivative i += 1 end return x₀ end If I extend my Context “algebra” with some more elementary operations\nBase.sin(d ::Context) = Context(sin(d.v), cos(d.v) * d.∂) Base.cos(d ::Context) = Context(cos(d.v), - sin(d.v) * d.∂) then I can try solving, say, trigonometric equations:\njulia\u0026gt; root_solve(x -\u0026gt; sin(x) - cos(x), 0.0) # find a root of sin(x) = cos(x) starting from x₀ = 0 [1]: f(0.0) = -1.0 [2]: f(1.0) = 0.30116867893975674 [3]: f(0.782041901539138) = -0.004746462127804163 [4]: f(0.7853981759997019) = 1.7822277875723103e-8 [5]: f(0.7853981633974483) = -1.1102230246251565e-16 0.7853981633974483 julia\u0026gt; π/4 0.7853981633974483 Nice. I find this all very satisfying. Especially given how few lines of Julia actually went into the final implementation.\nFor an industrial-strength implementation of this approach to AD, check out ForwardDiff.jl.\n  Summary  Almost all Julia operators are actually functions. Working in Julia often means being function-oriented. It is possible to intercept Julia functions with custom types and methods. As an example, this approach is one easy way to implement forward mode of automatic differentiation essentially from scratch. Being able to route execution to the right method based on the runtime types of all function arguments is multiple dispatch. Such routing is exponentially more expressive that single dispatch typical of the “classic” OOP. Sometimes multiple dispatch can get a little out of hand due to proliferation of methods that differ only in small details. Julia has some support for typical tasks like common type conversions and promotions needed for practical implementions of arithmetic operations over custom types.    I am abusing the partial derivative symbol \\(\\partial\\) for all my derivative notation here, including Julia code.↩\n My implementation ignores possible overflows in x², factorial(k), etc – it is intentionally made to evoke the symbolic series expression. A.k.a. “research code”.😄 ↩\n The compiler may choose to optimize this subexpression to \\(2x\\) but that doesn’t invalidate the forgoing.↩\n There are exceptions like \u0026amp;\u0026amp; and || because those obey short-circuit evaluation rules.↩\n Yes, Julia uses * for string concatenation. A little unorthodox?↩\n Although in general AD isn’t guaranteed to work with all such functions.↩\n   ","date":1572134400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572134400,"objectID":"a6ca1797119c6bf6e8b99e4cbb8bc39e","permalink":"/tutorials/study_julia_with_me/multiple_dispatch/","publishdate":"2019-10-27T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/multiple_dispatch/","section":"tutorials","summary":"I mentioned Julia’s fondness for multiple dispatch in my first typing tutorial. On purpose at the time, my case study was limited to the simplest case: single dispatch. I’ve waited until I ran across an example of a “natural”\" problem where multiple dispatch would play an important role. I think I’ve found one: differentiating a function with no closed-form formula.\nPreview  Play with a custom number-like type. Simplify implementation by leveraging Julia’s support for type promotions.","tags":null,"title":"Automatic differentiation from scratch, in 10 lines of Julia","type":"docs"},{"authors":null,"categories":null,"content":" Whenever picking up a new programming language with any kinds of \u0026ldquo;objects\u0026rdquo; I typically want to know quickly how it deals with notions of \u0026ldquo;object equality\u0026rdquo;. If the language also has \u0026ldquo;dictionaries\u0026rdquo; then there could be related subtleties in using objects as safe hash keys. These aspects of a language can lead to rare but hard-to-debug problems if not understood properly. It turns out that in Julia there are some slightly novel twists on this classic topic.\nPreview  Replay a puzzling object comparison example from an earlier tutorial. Meet and understand three different \u0026ldquo;comparators\u0026rdquo; in Julia, including egal.  Some structs are more equal than others In an earlier Julia typing tutorial I had the following example, repeated here with minor edits:\nCompare\njulia\u0026gt; struct Item a::Int; b::Int end julia\u0026gt; i1 = Item(1, 2) Item(1, 2) julia\u0026gt; i2 = Item(1, 2) Item(1, 2) julia\u0026gt; i1 === i2 # identical true julia\u0026gt; i1 == i2 # equal true  with\njulia\u0026gt; mutable struct MutableItem a::Int; b::Int end # mutable version of 'Item' julia\u0026gt; mi1 = MutableItem(1, 2) MutableItem(1, 2) julia\u0026gt; mi2 = MutableItem(1, 2) MutableItem(1, 2) julia\u0026gt; mi1 === mi2 # ok, not idential... false julia\u0026gt; mi1 == mi2 # also not equal??? false  The only difference between Item and MutableItem is the latter is a mutable variant of the former and yet that appears to change the results of both identity (===) and equality (==) queries. I don\u0026rsquo;t know about you, but the latter case, equality, was more surprising to me: why wouldn\u0026rsquo;t mi1 and mi2 be equal if i1 and i2 are \u0026ndash; after all, the \u0026ldquo;object content\u0026rdquo; is the same in each case?\nTo get a better idea of what\u0026rsquo;s going on I needed to look carefully at Julia\u0026rsquo;s definitions of \u0026ldquo;identity\u0026rdquo; and \u0026ldquo;equality\u0026rdquo;.\nObject identity or value equality? Many languages with \u0026ldquo;objects\u0026rdquo; allow me to take memory addresses of objects and essentially consider such addresses as permanent \u0026ldquo;identities\u0026rdquo; of the said objects. Think of id() in Python or \u0026amp; in C++. In languages that have reasons to make it difficult to obtain objects\u0026rsquo; addresses1, one could think of variables/struct fields as opaque \u0026ldquo;pointers\u0026rdquo; to objects. If I haven\u0026rsquo;t provided my own \u0026ldquo;oid\u0026rdquo; field for a struct, there really isn\u0026rsquo;t much else that can be done. Basically, such languages concede that \u0026ldquo;objects\u0026rdquo; must live somewhere in computer memory and don\u0026rsquo;t mind exposing addresses or other handles to their memory locations. Importantly, a replica of an object would necessarily have to live at a different address and hence would be \u0026ldquo;distinguishable\u0026rdquo; from the original. Many of us are so accustomed to this that we take it for granted.\nJulia designers saw an opportunity to re-examine the entire concept of \u0026ldquo;equality\u0026rdquo; and they took it. Julia has (at least) two comparison operators, === and ==. (They are also functions.)\n=== is (non-overloadble) \u0026ldquo;egal\u0026rdquo; Operator === implements the egal notion of equality described in Henry Baker\u0026rsquo;s seminal paper \u0026ldquo;Equal Rights for Functional Objects or, The More Things Change, The More They Are the Same\u0026rdquo;: x === y is true if x and y are programmatically indistiguishable, i.e. it is impossible to write a program that demonstrates any differences between x and y. This operator is a practical implementation of equivalence relation in Julia, as emphasized by Unicode ≡ being a synonym for ===:\njulia\u0026gt; i1 ≡ i2 # equivalent true  The reason object\u0026rsquo;s mutability becomes enmeshed in the concept of equality is because mutation is one option for telling objects apart: a program could try to mutate the value it sees via the x \u0026ldquo;handle\u0026rdquo; and look for the same change as seen via y: if the change is visible then x and y are really \u0026ldquo;the same thing\u0026rdquo;. Thus, mutable x and y would only be considered \u0026ldquo;egal\u0026rdquo; if they are truly pointers to the same address. On the other hand, if x and y are immutable and happen to point to the same data they are considered indistinguishable irrespective of whether they point to the same address under the covers2.\n In Julia, object mutability and equality/equivalence are closely intertwined.   === is a built-in operator that cannot be user-defined, therefore it must work for all possible types, built-in and custom. What should it do for composite types? As already discussed, if the objects being compared are immutable, their contents need to be compared and if the objects\u0026rsquo; type is composite there may be a need to recurse into individual fields for a \u0026ldquo;deep\u0026rdquo; content comparison. The rules that cover all situations correctly are:\n for \u0026ldquo;plain data\u0026rdquo; types like Int64 or Float64, compare them in a bitwise fashion; for composite mutable x and y, compare their memory addresses; for composite immutable x and y, compare their types first and if those are the same, apply === recursively on their component fields.  This explains the difference between i1 === i2 and mi1 === mi2 above: it is all indeed due to the type mutability and is by design. And I think this also helps explain why in Julia type mutability is escalated to the level of type definitions.\nIn addition to higher conceptual clarity, Baker\u0026rsquo;s definition of equality enables potentially higher runtime performance: indistinguishable objects can be freely copied or shared across different expressions, different threads, etc, allowing compiler optimizations that otherwise would be inhibited by \u0026ldquo;aliasing\u0026rdquo;. To quote Baker:\n If programming languages distinguish functional/immutable objects from non-functional/mutable objects, and if programs utilize a \u0026ldquo;mostly functional\u0026rdquo; style, then such programs will be efficient even in a non-shared-memory (\u0026ldquo;message-passing\u0026rdquo;) implementation.\n This seems to be well in keeping with Julia\u0026rsquo;s somewhat-functional nature combined with lofty performance goals.\n== is (overloadable) \u0026ldquo;value equality\u0026rdquo; Operator ==, on the other hand, provides potentially user-customized notion of \u0026ldquo;intuitive equality\u0026rdquo;. For example, integer 1 and floating point 1.0 are perceived as numerically equal in many user contexts even though they are of different types:\njulia\u0026gt; typeof(1), typeof(1.0) (Int64, Float64) julia\u0026gt; 1 == 1.0 true  In Julia this intuition extends to rational, complex, and other numbers:\njulia\u0026gt; map(typeof, (1 + 0im, 1//1, BigInt(1))) (Complex{Int64}, Rational{Int64}, BigInt) julia\u0026gt; 1 == 1 + 0im == 1//1 == BigInt(1) true  Such \u0026ldquo;across-types\u0026rdquo; equality is only allowed when it is mathematically exact, however:\njulia\u0026gt; 1/3 == 1//3 # 0.3333... can never be exactly equal to \u0026quot;one third\u0026quot; false julia\u0026gt; 1/2 == 1//2 # 0.5 can be represented exactly in 64 bits of IEEE 754 format, true julia\u0026gt; 1/10 == 1//10 # ...but 0.1 can't be (since mantissa is not a sum of powers of 2) false  Furthermore, numerical equality brings with it the usual IEEE 754 floating point complications:\njulia\u0026gt; NaN === NaN # one NaN is like any other NaN, true julia\u0026gt; NaN == NaN # ... but NaNs don't equal to anything, even themselves false julia\u0026gt; 0.0 === -0.0 # positive and negative 0.0 are distinguishable in IEEE 754, false julia\u0026gt; 0.0 == -0.0 # ...but they are '=='-equal true  Also unlike ==, value equality isn\u0026rsquo;t guaranteed to always return Bool. For example, if there is no \u0026ldquo;value\u0026rdquo; to compare to begin with:\njulia\u0026gt; 1 == missing missing julia\u0026gt; missing == missing missing  (for missing operands, == implements three-valued logic, similar to NULL in SQL.)\nBecause \u0026ldquo;value equality\u0026rdquo; is inherently highly type-dependent, == may need to be customized (overloaded). It is not something that Julia can do automatically for every possible user type. What it does do, though, is provide default behavior that is very conservative and safe:\n if not overloaded, default to === (see the definition in operators.jl):\n==(x, y) = x === y  for base types like Int64, Float64, String, etc provide overloads that support \u0026ldquo;intuitive\u0026rdquo; value equality as we are used to from other langauges: based on numerics, string content, etc.\n  And now the rest of the Item/MutableItem puzzle is clear: because == falls back to === it will do whatever \u0026ldquo;egal\u0026rdquo; concept must do, which in turn depends on whether the type is mutable. For Item it happens to be what we would expect: recurse into and compare fields a and b. To make MutableItem compare the same way I would need to add an explicit == overload myself3:\njulia\u0026gt; import Base julia\u0026gt; Base.:(==)(lhs ::MutableItem, rhs ::MutableItem) = (lhs.a == rhs.a) \u0026amp;\u0026amp; (lhs.b == rhs.b) julia\u0026gt; mi1 == mi2 # now equal true  Oh, no! What is isequal()? An eagle-eyed reader of Julia documentation for == would have noticed mentions of something called isequal():\n Use isequal or === to always get a Bool result.\n It is not a \u0026ldquo;functionally named\u0026rdquo; equivalent of == as you might have thought (and I did at first, reacting to some subconscious stylistic similarities to the likes of isless() in C++). No, it is == \u0026ldquo;fixed\u0026rdquo; to deal with idiosyncrasies in ==-comparison applied to floating point values (-0.0, NaN) and missing:\n Similar to ==, except for the treatment of floating point numbers and of missing values. isequal treats all floating-point NaN values as equal to each other, treats -0.0 as unequal to 0.0, and missing as equal to missing. Always returns a Bool value.\n Why is this necessary? Primarily to be able to use types with floating point/missing content as Dict keys:\n isequal is the comparison function used by hash tables (Dict). isequal(x,y) must imply that hash(x) == hash(y).\nThis typically means that types for which a custom == or isequal method exists must implement a corresponding hash method (and vice versa).\n Dictionaries are ubiquituous in Julia and the language is designed so that anything4 could be used as a dictionary key. Having a hashtable comparator that might return missing is unacceptable. Likewise unacceptable is a value that can never be equal to itself (NaN).\nUnsurprisingly enough, isequal() defaults to == except when encountering floating point/missing values and in fact does not need to be overloaded unless the latter is a runtime possibility. The situation is exactly parallel to, say how Java needs to customize Object.equals() for Doubles \u0026ndash; a practical, if not overly elegant, solution.\n I\u0026rsquo;ve found the following delegation chain helpful to remember: the default implementation of isequal() calls == which in turn (if not overloaded) defaults to ===.   Conclusion Let me put behavioral traits for the three (so far) comparison operations in Julia together for easy comparison:\n   Operation Overridable? Synonyms Return type Fallback     isequal() yes  Bool ==   == yes  Bool or Missing ===   === no ≡ Bool     And of course, it is always important to remember the role played by type immutability.\n For example, because pointer arithmetic is \u0026ldquo;bad\u0026rdquo; or because something like a garbage collector needs to know when an object becomes unreferenced and eligible for destruction. ^ Assuming, of course, that the language in question doesn\u0026rsquo;t expose other means of distinguishing memory locations, e.g. by figuring out their addresses. ^ There is ongoing debate in the community whether Julia could provide such recursive overloads automatically in certain \u0026ldquo;safe\u0026rdquo; cases, but there is no universal agreement on what all such cases would be. ^ Normally I would say that hashing floating point values is asking for trouble, but Julia thinks otherwise. ^   ","date":1573084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573084800,"objectID":"c8ff1fac34f1d38b787af64633836f5e","permalink":"/tutorials/study_julia_with_me/equality_vs_identity/","publishdate":"2019-11-07T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/equality_vs_identity/","section":"tutorials","summary":"Whenever picking up a new programming language with any kinds of \u0026ldquo;objects\u0026rdquo; I typically want to know quickly how it deals with notions of \u0026ldquo;object equality\u0026rdquo;. If the language also has \u0026ldquo;dictionaries\u0026rdquo; then there could be related subtleties in using objects as safe hash keys. These aspects of a language can lead to rare but hard-to-debug problems if not understood properly. It turns out that in Julia there are some slightly novel twists on this classic topic.","tags":null,"title":"Navigating through Julia's ==, ===, and isequal().","type":"docs"},{"authors":null,"categories":["julia"],"content":" Ever since my knapsack benchmark I’ve been curious why Julia was still \\(\\approx 50\\%\\) slower than either Java or C++ on what essentially were a couple of nested for-loops over a pair of arrays. I’ve done some exploring and now believe that my first guess was correct: much of the remaining Julia overhead is due to its bounds checking.\nEliding bounds checking unconditionally with @inbounds My first test was to isolate all opt_value() lines with array access into a begin...end block and annotate it with @inbounds:\n function opt_value(W ::Int64, items ::Array{Item}) ::Int64 ... @inbounds( begin V[items[1].weight:end] .= items[1].value ... for j in 2 : n V, V_prev = V_prev, V itemⱼ = items[j] for w in 1 : W V_without_itemⱼ = V_prev[w] ... end end end ) ... Here is the data from the original benchmark with added new timing measurements of the above version, labeled julia.ib:  Figure 1: Calculation time as a function of problem size. julia.ib labels data obtained from an @inbounds-annotated Julia implementation.  Nice! With bounds checks elided Julia performance is now in line with Java and C++: its underperformance has decreased to a maximum of \\(\\approx 15\\%\\). And Julia is now even the champion for small problem sizes.\n Peeking at differences in generated code via @code_native Another neat facility of Julia allows me to examine JIT-compiled code very easily and right from my REPL session:\njulia\u0026gt; d = Knapsack.make_random_data(5_000, 12345) (5000, Main.Knapsack.Item[Main.Knapsack.Item(609, 6198), ... julia\u0026gt; @code_native debuginfo=:none Knapsack.opt_value(d[1],d[2]) .text pushq %rbp movq %rsp, %rbp ... L1165: movl $2, %edx jmp L825 nopw (%rax,%rax) I don’t show the full asm listing because that’s not particularly useful unless you’re a low-level programmer. Comparing it with and without @inbounds does confirm, however, that the former version is shorter: about 200 instructions instead of 320.\nIt is more instructive to look at the compiled version(s) of a simpler function that simply extracts a single slot from an input array:\nfunction getit(a ::Array{Int64}) return a[123] end julia\u0026gt; @code_native getit(rand(Int64, 1000)) .text ; ┌ @ work.jl:185 within `getit\u0026#39; ; │┌ @ work.jl:185 within `getindex\u0026#39; cmpq $122, 8(%rdi) jbe L18 movq (%rdi), %rax movq 976(%rax), %rax ; │└ retq L18: pushq %rbp movq %rsp, %rbp ; │ @ work.jl:185 within `getit\u0026#39; ; │┌ @ array.jl:728 within `getindex\u0026#39; movq %rsp, %rax leaq -16(%rax), %rsi movq %rsi, %rsp movq $123, -16(%rax) movabsq $jl_bounds_error_ints, %rax movl $1, %edx callq *%rax nopl (%rax) ; └└ Note the instructions to check the fixed (and 0-based) index against the array input length (cmpq $122, 8(%rdi)) followed by a conditional jump to native C routine jl_bounds_error_ints() that creates and throws a BoundsError exception object. The following version of getit(), with the only array indexing expression annotated with @inbounds, is much shorter:\nfunction getit(a ::Array{Int64}) return @inbounds a[123] end julia\u0026gt; @code_native getit(rand(Int64, 1000)) .text ; ┌ @ work.jl:185 within `getit\u0026#39; ; │┌ @ work.jl:185 within `getindex\u0026#39; movq (%rdi), %rax movq 976(%rax), %rax ; │└ retq nopl (%rax,%rax) ; └  Disabling bounds checking from command line It seems that I can get disable bounds checking summarily using –check-bounds=no option:\n\u0026gt;julia --check-bounds=no Knapsack.jl The documentation is somewhat sparse, but it appears that when this option is used it will completely override all @inbounds annotations in the source code, i.e. either all checks are done in a mandatory fashion (“yes”) or none are (“no”). This could be used for “debug” vs “release” calculation runs. However, since bounds checking affects code as it’s being generated, some more documentation is needed to understand how this option works with “pre-compiled” packages and Julia “system” code. I also would like to understand the functionality and use cases for the @propagate_inbounds macro.\n ","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571249107,"objectID":"9ede5576d8eaddd7dce4683d7d8d6d61","permalink":"/post/20191016/checking-bounds-checking-in-julia/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/post/20191016/checking-bounds-checking-in-julia/","section":"post","summary":"Ever since my knapsack benchmark I’ve been curious why Julia was still \\(\\approx 50\\%\\) slower than either Java or C++ on what essentially were a couple of nested for-loops over a pair of arrays. I’ve done some exploring and now believe that my first guess was correct: much of the remaining Julia overhead is due to its bounds checking.\nEliding bounds checking unconditionally with @inbounds My first test was to isolate all opt_value() lines with array access into a begin.","tags":["performance"],"title":"Checking bounds checking in Julia","type":"post"}]