[{"authors":["admin"],"categories":null,"content":"Vlad Roubtsov works on stochastic and simulation-based optimization problems at a large retailer. He leverages his cross-disciplinary background (physics, math, computer science) to bridge quantitative research with software development.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"258be61fc6f4ec5204c804c5b2f71116","permalink":"/authors/vlad/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vlad/","section":"authors","summary":"Vlad Roubtsov works on stochastic and simulation-based optimization problems at a large retailer. He leverages his cross-disciplinary background (physics, math, computer science) to bridge quantitative research with software development.","tags":null,"title":"Vlad Roubtsov","type":"authors"},{"authors":null,"categories":null,"content":" Introduction and welcome Greetings! I work on stochastic optimization and simulation problems and happen to have extensive background in both \u0026ldquo;high productivity\u0026rdquo; and \u0026ldquo;high performance\u0026rdquo; programming languages. I had my first peek at Julia a few years ago. The concept was great and the new language certainly had a real, uncrowded niche to fit in: something that promised to be both high productivity and high performance. But right then it was brand new and the ecosystem just didn\u0026rsquo;t feel organized or stable enough for \u0026ldquo;production use\u0026rdquo;.\nA few years later, Julia v1.0 milestone has been achieved, there is an IDE or two, university classes are abandoning MATLAB/octave in favor of Julia, and new Julia books are coming out with increasing frequency. Now it appears to me that Julia is much more mature and ready for another, more thorough, examination. For this reason, I invite you to take part in this study group.\nPurpose What do we plan to get out of this study? A couple of things, at least:\n Julia does not look like a radically new programming language to me. Rather, it appears to combine many of the best and most modern aspects of language design in order to support productive scientific programming. I would us like to understand the main paradigms in Julia so that we can judge where and when it would be the most effective choice for research work.\n As a component of overall productivity, I care about language performance in both interactive and parallel/distributed settings. I would like us to put Julia speed claims to the test.\n Julia is still quite new (v1.0 just shipped in 2018) and good study material is a little hard to come by. We could look at some existing open-source tools for operations research and machine learning as examples of working Julia projects, and perhaps prototype a few of our own in the process.\n  Study agenda (open to feedback) At a high level, these are the topics I\u0026rsquo;d like us to cover (not necessarily in the order shown and subject to feedback and suggestions from the group):\n language design and capabilities: core types, generic functions/multiple dispatch, metaprogramming, parallel/distributed programming, etc. general purpose tooling: IDE, plotting, debugging, profiling, etc. native support and tools for OR, ML, and statistics: dataframes, built-in linear algebra, JuMP, JuML, etc.  Each week I would us like to formulate a practical question or need and investigate a solution (or lack thereof) within the Julia ecosystem, at which point I will document it here. Some example questions off the top of my head are:\n What\u0026rsquo;s this I hear about Julia being \u0026ldquo;fast\u0026rdquo;? How does it compare with Python or R? How about C++? I have an optimization problem that needs a custom algorithm not covered by off-the-shelf solvers \u0026ndash; should I consider a (pure) Julia implementation? Conversely, I have a bog-standard LP model \u0026ndash; is there still an advantage to working with it in Julia? How do I develop a Julia package, or, more generally, have workflow that\u0026rsquo;s good for research that I want to share with colleagues? Julia is marketed as good for parallel computing \u0026ndash; what\u0026rsquo;s the reality of this claim? \u0026hellip; and so on, as long as there are useful topics to explore.  Prerequisites I think it would be helpful to have some experience with one or two of the languages that Julia aims to (will eventually?) replace: R, Matlab, Python, and others of that ilk. Julia also borrows from JIT\u0026rsquo;ed languages like Java, so familiarity with those can only help.\n","date":1569024000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1569024000,"objectID":"7ee0ab098699d251db43779d8f3524d3","permalink":"/tutorials/study_julia_with_me/","publishdate":"2019-09-21T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/","section":"tutorials","summary":"Ongoing study of topics from language design, operations research, and computer/data sciences.","tags":null,"title":"Study Julia with me","type":"docs"},{"authors":null,"categories":null,"content":" Suggested software setup I have been using Julia successfully on MacOS and Linux (Fedora). I would suggest starting with a JuliaPro installer for this study. I\u0026rsquo;ve kept a few notes about my install process:\n I\u0026rsquo;ve been using v1.2 so far, both REPL and Juno \u0026ldquo;IDE\u0026rdquo;.\n on MacOS you will need to have XCode tools installed, see the install guide on Linux the install guide mentions needing these libs (install via yum/dnf/apt-get/etc, I only needed to add xclip):\nxclip libXScrnSaver   apparently, starting with v1.2 what\u0026rsquo;s included appears to have changed: to minimize the installer size the \u0026ldquo;curated list of packages\u0026rdquo; are no longer part of the download itself. They can be installed via the usual Pgk commands. What\u0026rsquo;s different for \u0026ldquo;curated\u0026rdquo; packages is that the install will be configured to use Julia Computing\u0026rsquo;s github repo1 so that only the supposedly tested versions are available. Using Julia Computing\u0026rsquo;s repo will require authenticating to get their token. Keep this in mind if you plan to play with packages outside of their supported list.\n for REPL you can use Juno\u0026rsquo;s \u0026ldquo;REPL\u0026rdquo; tab. I find that doing so gives me experince similar to that of RStudio (a good thing):\n  { width=80% }\n if you prefer not to rely on the IDE (which is not that great right now) and use REPL (which is very functional), you can use julia binary from your JuliaPro distribution:  MacOS: /Applications/JuliaPro-\u0026lt;version\u0026gt;.app/Contents/Resources/julia/Contents/Resources/julia/bin/julia Linux: \u0026lt;install dir\u0026gt;/Julia/bin/julia   FAQs and tips for fellow Julia explorers General  if you\u0026rsquo;re working with custom types, it is recommended to keep work code in a module (which could be as simple as keeping it inside a module MyModule ... end block) even for one-off stuff: onlike, say, R or Python Julia currently does not allow updating type definitions within a session without restarting it. It seems to have gone through several iterations of addressing this workflow need, but right now re-include()ing a module seems to be what\u0026rsquo;s guaranteed to work.\n there is also Revise.jl (which I haven\u0026rsquo;t tried yet) see this discussion for more color  another reason for working inside a module is because code will run faster (it will be JIT\u0026rsquo;ed sooner)\n  Juno Juno is basically a few packages inside Atom. It is not quite an \u0026ldquo;IDE\u0026rdquo; at this point. Visual Studio Code might be a reasonable alternative, but in my experiments VSC had trouble with Julia v1.2. Juno comes bundled with a julia build from the same entity.\n as a result of being a collection of Atom packages, some things you might want to tweak in the UI could be dispersed over multiple places. For example:  \u0026ldquo;Julia Client\u0026rdquo; package:  you may wish to choose positioning of various tabs: Workspace, Documentation, Plots, REPL, etc  \u0026ldquo;tool-bar\u0026rdquo; package:  you might want to opt for smaller icons  \u0026ldquo;tree-view\u0026rdquo; package:  \u0026ldquo;Always Open Existing\u0026rdquo;, \u0026ldquo;Auto Reveal\u0026rdquo; settings might be of interest    Books, other resources There is a constantly growing list of resources at https://julialang.org. I list below some resources that I\u0026rsquo;ve either used myself or that seemed to stand out from the rest.\nbooks about or based on Julia v1.0+:  \u0026ldquo;Think Julia: How to Think Like a Computer Scientist\u0026rdquo; by Ben Lauwens and Allen Downey.  this is the only \u0026ldquo;pure computer science\u0026rdquo; Julia book in my list.  \u0026ldquo;A Deep Introduction to Julia for Data Science and Scientific Computing\u0026rdquo; by Chris Rackauckas.  the author is very active in Julia space; this material seems very good (if you\u0026rsquo;re ok with notebooks).  \u0026ldquo;Statistics with Julia: Fundamentals for Data Science, Machine Learning and Artificial Intelligence\u0026rdquo; by Hayden Klok and Yoni Nazarathy (2019 draft PDF free from the authors).  based on a statistics course at the University of Queensland; Julia crash course in Chapter 1 and a handy \u0026ldquo;How-to\u0026rdquo; in Appendix A.  \u0026ldquo;Julia Programming for Operations Research, 2nd ed.\u0026rdquo; by Changhyun Kwon.  another Julia crash course chapter; JuMP workship.   reference docs:  \u0026ldquo;Julia 1.2 Documentation\u0026rdquo;, HTML and PDF. \u0026ldquo;Introducing Julia\u0026rdquo; Wikibook, a nice complement to the language manual.  video lectures, talks:  \u0026ldquo;Intro to Julia 1.0\u0026rdquo; by Jane Herriman of Julia Computing. Videos from JuliaCon 2019 \u0026ndash; some good stuff, particularly by Julia creators.  online practice:  Julia Box from Julia Computing has a free plan. Julia exercism track looks like a good collection of exercises   Julia packages are maintained as git repos. ^   ","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569196800,"objectID":"e744053bcdda3203a21dab248ba3b320","permalink":"/tutorials/study_julia_with_me/structure/","publishdate":"2019-09-23T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/structure/","section":"tutorials","summary":"Suggested software setup I have been using Julia successfully on MacOS and Linux (Fedora). I would suggest starting with a JuliaPro installer for this study. I\u0026rsquo;ve kept a few notes about my install process:\n I\u0026rsquo;ve been using v1.2 so far, both REPL and Juno \u0026ldquo;IDE\u0026rdquo;.\n on MacOS you will need to have XCode tools installed, see the install guide on Linux the install guide mentions needing these libs (install via yum/dnf/apt-get/etc, I only needed to add xclip):","tags":null,"title":"","type":"docs"},{"authors":null,"categories":null,"content":" Every once in a while, you need to try an algorithm that’s “custom enough” that it must be coded directly in your “research language”. Perhaps the existing “high-performance” libs don’t support your desired variant or it’s too much setup work switching to another language just to prototype a quick one-off idea. This an instance of the “two language problem” that’s been the reality of technical computing for decades.\nUsers of R, Python, and similar “high-productivity” environments know that native control flow constructs (for-loops, etc) in those languages kill performance unless they can be expressed in “vectorized” form. One big attraction of Julia to me is that it claims to fix this inconvenience. Let’s have a look.\nPreview  Julia, Python, Java, and C++ are compared for implementing the same iterative algorithm (knapsack solver). The implementation does virtually no memory allocation, so what’s being tested is the speed of looping and array access. Julia is fast (although not quite as fast as C++ or Java). Python, however, comes out looking horribly slow by comparison.   Benchmark problem For a simple yet realistic test of performance let’s code a solution to the 0/1 knapsack problem. I am sure you’ve heard of it – it is a classic discrete optimization problem in its own right and shows up as a building block within many optimization algorithms.\nAs a reminder, the problem is to pack a knapsack of finite weight capacity with a chosen set of items of varying value so as to maximize the total packed value:\n\\[ \\begin{equation*} \\begin{aligned} \u0026amp; \\underset{x}{\\text{maximize}} \u0026amp; \u0026amp; V = \\sum_{i=1}^n v_i x_i \\\\ \u0026amp; \\text{subject to} \u0026amp; \u0026amp; \\sum_{i=1}^n w_i x_i \\leq W \\\\ \u0026amp; \u0026amp; \u0026amp; x_i \\in \\{0,1\\} \\text{ for all } i \\text{ in } \\{1,\\dots,n\\} \\end{aligned} \\end{equation*} \\] We have \\(n\\) items of integral1 weights \\(w_i \u0026gt; 0\\) and value \\(v_i\\). \\(W\\) is the knapsack weight capacity. Each item is either chosen or not, hence the “0/1” in the name.\nSince all \\(x_i\\) are restricted to be binary (boolean), this is an integer linear problem with a search space size of \\(2^n\\). It can be given to an MILP solver. But because of a simple nested structure the problem also has a straightforward dynamic programming (DP) solution that I can code in a few lines.\n Dynamic programming algorithm Assume the problem has been solved for all knapsack capacities up to some \\(w\\) while making use of only a subset of items \\(1, \\dots, j\\), and let \\(V(w, j)\\) be the solution, i.e. the maximum achievable knapsack value. Now consider all smaller subproblems without item \\(j\\), \\(V(w\u0026#39;, j-1)\\) for all possible \\(w\u0026#39;\\). \\(V(w, j)\\) and \\(V(w\u0026#39;, j-1)\\) are connected with a single “move” of looking at item \\(j\\) and deciding to either include it in the optimal selection for knapsack \\(V(w, j)\\) or not:\n if including item \\(j\\) is a move that’s feasible (\\(w_j \\leq w\\)) then \\(V(w, j)\\) is the best of \\(v_j + V(w\u0026#39;, j - 1)\\) and \\(V(w\u0026#39;, j - 1)\\) depending on whether the move is “optimal” (improves \\(V\\)) or not; in the former case \\(w\u0026#39; = w - w_j\\) and in the latter \\(w\u0026#39; = w\\); if including item \\(j\\) is not feasible (\\(w_j \u0026gt; w\\)) then \\(V(w, j) = V(w\u0026#39;, j - 1)\\) and \\(w\u0026#39; = w\\); \\(V(w, 1)\\) is 0 or \\(v_1\\) depending on whether the first item fits within capacity \\(w\\).  In other words, knapsack subproblems have a recurrent relationship for \\(j \u0026gt; 1\\)\n\\[ V(w, j) = \\left\\{ \\begin{array}{@{}ll@{}} V(w , j - 1) \u0026amp; \\text{if}\\ w \\lt w_j, \\\\ \\max \\big\\{v_j + V(w - w_j, j - 1),V(w , j - 1)\\big\\} , \u0026amp; \\text{otherwise} \\\\ \\end{array} \\right. \\] together with a boundary condition for \\(j = 1\\) \\[ V(w, 1) = \\left\\{ \\begin{array}{@{}ll@{}} 0 \u0026amp; \\text{if}\\ w \\lt w_1, \\\\ v_1, \u0026amp; \\text{otherwise} \\\\ \\end{array} \\right. \\]\n(The above could be stated more compactly if \\(j\\) started at 0 and \\(V(w,0)\\) were defined as 0.)\nTo capture inputs into a problem instance, I will use an array of Julia structs. Although I could get away with a list or a tuple, I think a struct makes for more readable example code:\nstruct Item value ::Int64 weight ::Int64 end If \\(V(w, j)\\) is represented as a matrix (another first-class citizen in Julia), a non-recursive implementation would be to sweep the space of \\(w\\) and \\(j\\) going from smaller to larger item sets:\nfunction opt_value(W ::Int64, items ::Array{Item}) ::Int64 n = length(items) # V[w,j] stores opt value achievable with capacity \u0026#39;w\u0026#39; and using items \u0026#39;1..j\u0026#39;: V = Array{Int64}(undef, W, n) # W×n matrix with uninitialized storage # initialize first column v[:, 1] to trivial single-item solutions: V[:, 1] .= 0 V[items[1].weight:end, 1] .= items[1].value # do a pass through remaining columns: for j in 2 : n itemⱼ = items[j] for w in 1 : W V_without_itemⱼ = V[w, j - 1] V_allow_itemⱼ = (w \u0026lt; itemⱼ.weight ? V_without_itemⱼ : (itemⱼ.value + (w ≠ itemⱼ.weight ? V[w - itemⱼ.weight, j - 1] : 0))) V[w, j] = max(V_allow_itemⱼ, V_without_itemⱼ) end end return V[W, n] end That loop is almost a straightforward translation of my recurrence, although the body needs some extra edge condition checks. Note the familiar ?: syntax for ternary operator and some comforting exploitation of Julia’s support for Unicode math symbols.\nThis may not be the best bit of Julia code you’ll ever see, but I’ve literally just learned enough syntax for my first experiment here. One performance-related concession to Julia has already been included above by arranging to traverse \\(V\\) in column-major order. Yes, Julia opts for R/MATLAB/Fortran design choice here and that is not under end user control, as far as I can tell at this point. Right now, it does feel like R and yet awkwardly different from C++, which would be my production choice for this particular problem.\n An obvious improvement Notice also that I cheat a little and do not compute half of the solution, specifically the optimal \\(x_i\\)’s2. This is to keep the experiment simple and focus on iterative performance only. However, since this means I don’t need to capture the optimal solution structure in \\(V\\) matrix, I don’t need to allocate it: it is sufficient to allocate a couple of buffers for columns \\(j\\) and \\(j-1\\) and re-use them as needed:\nfunction opt_value(W ::Int64, items ::Array{Item}) ::Int64 n = length(items) V = zeros(Int64, W) # single column of size W, zero-initialized V_prev = Array{Int64}(undef, W) # single column of size W, uninitialized storage V[items[1].weight:end] .= items[1].value for j in 2 : n V, V_prev = V_prev, V itemⱼ = items[j] for w in 1 : W V_without_itemⱼ = V_prev[w] V_allow_itemⱼ = (w \u0026lt; itemⱼ.weight ? V_without_itemⱼ : (itemⱼ.value + (w ≠ itemⱼ.weight ? V_prev[w - itemⱼ.weight] : 0))) V[w] = max(V_allow_itemⱼ, V_without_itemⱼ) end end return V[W] end This version will now scale to quite large problem instances.\n Algorithm speed: Julia vs Python vs Java vs C++ To measure performance, I use the @elapsed macro from Julia’s family of @time and friends with randomly constructed problem instances of different scale. Because Julia is JIT-based I need to be careful and do a few timing repeats after burning the very first measurement.\nActually, I’ll use the median of 5 repeats which is even more robust:\nfunction run(repeats = 5) @assert repeats \u0026gt; 1 times = zeros(Float64, repeats) seed = 12345 for W in [5_000, 10_000, 20_000, 40_000, 80_000] for repeat in 1 : repeats spec = make_random_data(W, seed += 1) times[repeat] = @elapsed opt_value(spec[1], spec[2]) end sort!(times) println(W, \u0026quot;, \u0026quot;, times[(repeats + 1) ÷ 2]) end end For the promised language shootout, I’ve implemented this same algorithm in three other languages: Python, Java, and C++. I event went as far as make sure that all “ports” used similar array data types and even the exact same random number generator for building the exact same random problem instances. To maintain consistency with statically typed languages (Java, C++), I ensure that value types used by the dynamic versions are 64-bit integers as much as possible – this required some minor contortions in Python.\nHere’s the C++ version of opt_value() (you can get all four language versions from this github repo, there is a single source file per language that’s trivial to compile and run):\nint64_t opt_value (int64_t const W, std::vector\u0026lt;item\u0026gt; const \u0026amp; items) { auto const n = items.size (); std::unique_ptr\u0026lt;int64_t []\u0026gt; const v_data { std::make_unique\u0026lt;int64_t []\u0026gt; (W) }; // \u0026quot;zeros\u0026quot; std::unique_ptr\u0026lt;int64_t []\u0026gt; const v_prev_data { std::make_unique\u0026lt;int64_t []\u0026gt; (W) }; // \u0026quot;zeros\u0026quot; int64_t * V { v_data.get () }; int64_t * V_prev { v_prev_data.get () }; for (int64_t w = items[0].weight; w \u0026lt;= W; ++ w) V[w - 1] = items[0].value; for (std::size_t j = 1; j \u0026lt; n; ++ j) { std::swap (V, V_prev); item const \u0026amp; item { items [j] }; for (int64_t w = 1; w \u0026lt;= W; ++ w) { auto const V_without_item_j = V_prev[w - 1]; auto const V_allow_item_j = (w \u0026lt; item.weight ? V_without_item_j : (item.value + (w != item.weight ? V_prev[w - 1 - item.weight] : 0))); V[w - 1] = std::max(V_allow_item_j, V_without_item_j); } } return V[W - 1]; } I’ve tried to keep the implementation consistent and idiomatic within each language. (It helps that my algorithm is basically a couple of nested loops without memory allocation, some arithmetic, and not much else.) All these implementations could in principle be tuned further using language-specific techniques, but that’s not my goal here.\nHere is a snippet of calculation latency data for \\(W=80000\\), captured on the exact same CPU:\n  lang  W  time (sec)      julia  80000  0.1135    python  80000  28.2094    java  80000  0.0781    c++  80000  0.0710     Hmm. Perhaps you begin to suspect that Python is not going to win this. Here is a log-log plot of all data, for all \\(W\\)’s:\n Figure 1: Calculation time as a function of knapsack problem size. (Note that both axes use log scale.)  Julia is about 50% slower than either Java or C++. But it is Python that is the real laggard in the group: slower by more than 100x across the entire range of tested problem sizes. Ouch!\nGoing back to the matrix version of the algorithm, you can see how the matrix is accessed predictably over \\(j\\) columns but somewhat randomly (in a data-depedent fashion) over \\(w\\) rows. There doesn’t seem to be a way to express the algorithm in linear algebra operations. This difficulty is retained in the optimized two-column-buffer version. The algorithm is short and simple but there just doesn’t seem to be a way to vectorize it so that it would be fast, say, in numpy. Massive underperformance of the Python version is the cost I pay for slow interpretation of Python bytecode.\nAs for Julia, so far so good. I might just get used to the “no need to vectorize code to make it fast” lifestyle.\n  Restricting weights to be integral is not crucial to my test here but could make the problem statement acceptable to more solver types.↩\n Some might argue that \\(x_i\\)’s are in fact more than half of the solution. Ok, so I cheat a lot.↩\n   ","date":1569974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569974400,"objectID":"b8c7a1dfd86e76bae738e1a867b98081","permalink":"/tutorials/study_julia_with_me/knapsack_benchmark/","publishdate":"2019-10-02T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/knapsack_benchmark/","section":"tutorials","summary":"Every once in a while, you need to try an algorithm that’s “custom enough” that it must be coded directly in your “research language”. Perhaps the existing “high-performance” libs don’t support your desired variant or it’s too much setup work switching to another language just to prototype a quick one-off idea. This an instance of the “two language problem” that’s been the reality of technical computing for decades.\nUsers of R, Python, and similar “high-productivity” environments know that native control flow constructs (for-loops, etc) in those languages kill performance unless they can be expressed in “vectorized” form.","tags":null,"title":"How do you say \"0/1 knapsack\" in four languages?","type":"docs"},{"authors":null,"categories":null,"content":" Julia is unusual in that it is a dynamically typed language (meaning you don\u0026rsquo;t have to declare variable types \u0026ldquo;statically\u0026rdquo;, in program text), while at the same time supporting a rich mechanism for communicating type information to the JIT compiler and runtime. Effectively, Julia has both dynamic and static typing. When provided, static type information can have non-trivial impact on Julia code behavior, both in terms of functionality and performance.\nThis distinctive typing seems to be one of the defining features of Julia. To keep tutorials to a reasonable length, I cover basics here and delay further case studies (parameterized types, overloading, multiple dispatch) until the next one.\nPreview  A quick tour of Julia type taxonomy. Drill into type annotations and understand their impact on: code documentation, correctness, performance. Julia functions are always \u0026ldquo;virtual\u0026rdquo;. Julia supports functional OOP as a paradigm.  Types and inheritance Everything in Julia that is a value also has a type and the types are first-class objects (think T.class in Java or T.__type__ in Python). Subtyping relationships are set up and queried with the \u0026lt;: subtype operator:\njulia\u0026gt; typeof(1) Int64 julia\u0026gt; supertype(typeof(1)) Signed julia\u0026gt; typeof(1) \u0026lt;: supertype(typeof(2)) true julia\u0026gt; Int64 \u0026lt;: Signed true julia\u0026gt; Int64 \u0026gt;: Signed # there is also a 'supertype operator', used trivially here but more useful for parameterized types false  Types known to a given runtime session form a tree (a directed graph, actually) rooted at Any.\nSpeaking of [type] trees I found it handy to explore subtrees within this graph of types using the following helper function. It builds on subtypes() from InteractiveUtils.jl, which would be already imported and availalbe if you\u0026rsquo;re in REPL but may need an import in a script:\nimport InteractiveUtils function subtypetree(T, depth = 0) # you might also want to add 'max_depth'... println('\\t' ^ depth, T) for t in InteractiveUtils.subtypes(T) subtypetree(t, depth + 1) end end  Arithmetic types There is a collection of (ahem) typical types for arithmetic:\n Int8, \u0026hellip;, Int64, Int128, plus unsigned variants, Float16, Float32, Float64, BigInt and BigFloat,  organized in this hierarchy:\njulia\u0026gt; subtypetree(Number) Number Complex Real AbstractFloat BigFloat Float16 Float32 Float64 AbstractIrrational Irrational Integer Bool Signed BigInt Int128 Int16 Int32 Int64 Int8 Unsigned UInt128 UInt16 UInt32 UInt64 UInt8 Rational  The first thing that struck me here is Julia\u0026rsquo;s keeping with its emphasis on performance: Julia integers are not \u0026ldquo;big\u0026rdquo; by default (contrast with Python 3). And the rich spectrum of arithmetic bit widths is meaningful: Float16 and Float32 could be useful for GPU computing, while wide Int-types could work with SSE/AVX/etc instructions and/or support efficient interfacing with native code. (I say \u0026ldquo;could\u0026rdquo; because I have no idea yet if that\u0026rsquo;s really the case.)\nPrimitive types Turns out all of these arithmetic types are not what you\u0026rsquo;d call \u0026ldquo;built into the compiler\u0026rdquo; but are rather defined in the language itself, as standard primitive types1:\nprimitive type Bool \u0026lt;: Integer 8 end primitive type Int64 \u0026lt;: Signed 64 end  The syntax is\nprimitive type «name» \u0026lt;: «supertype» «bits» end  where the supertype is optional (and defaults to Any). I can apparently define my own primitive type, a 24-bit integer2:\njulia\u0026gt; primitive type Int24 \u0026lt;: Signed 24 end julia\u0026gt; subtypetree(Number) Number Complex Real ... Integer Bool Signed ... Int24 ...  (This is great\u0026hellip; but how do I construct an Int24 or define arithmetic? Julia docs are not clear on that \u0026ndash; I think I know how to proceed but that is outside of today\u0026rsquo;s scope.)\nConcrete vs abstract types I\u0026rsquo;ve already used a struct in the knapsack benchmark to represent knapsack items as instances of this type:\nstruct Item value ::Int64 weight ::Int64 end  These are straightforward: Items contain fields (the type is composite) and can be instantiated (the type is concrete). They can also be introspected:\njulia\u0026gt; sizeof(Item) 16 julia\u0026gt; fieldcount(Item) 2 julia\u0026gt; fieldnames(Item) (:value, :weight) julia\u0026gt; fieldtypes(Item) (Int64, Int64) julia\u0026gt; function showfields(T) for i in 1 : fieldcount(T) println(fieldoffset(T, i), '\\t', fieldname(T, i), \u0026quot;\\t::\u0026quot;, fieldtype(T, i)) end end showfields (generic function with 1 method) julia\u0026gt; showfields(Item) 0 value ::Int64 8 weight ::Int64  but a few things here are less obvious:\n structs default to being immutable unless explicitly marked as mutable. they are always final, i.e. cannot be further inherited from3. (This is also true of primitive and, in fact, any non-abstract types.)  Defaulting to immutability is not an arbitrary choice. It can be exploited by the compiler to optimize performance and memory usage by \u0026ldquo;interning\u0026rdquo; values that are indistinguishable if they are equal. Compare\njulia\u0026gt; i1 = Item(1, 2) Item(1, 2) julia\u0026gt; i2 = Item(1, 2) Item(1, 2) julia\u0026gt; i1 == i2 # equal true julia\u0026gt; i1 === i2 # actually, the same \u0026quot;interned\u0026quot; object true julia\u0026gt; i1 == Item(1, 3) false  with\njulia\u0026gt; mutable struct MutableItem # mutable version of 'Item' value ::Int64 weight ::Int64 end julia\u0026gt; i1 = MutableItem(1, 2) MutableItem(1, 2) julia\u0026gt; i2 = MutableItem(1, 2) MutableItem(1, 2) julia\u0026gt; i1 == i2 # not equal! false # \u0026lt;- surprised? looks like '==' needs to be defined for custom mutable types...  Julia also has abstract types which can\u0026rsquo;t be instantiated, but are instead used to organize the type graph via shared parent nodes. You could also say they act as \u0026ldquo;marker\u0026rdquo; or \u0026ldquo;trait\u0026rdquo; base classes, like Number or Signed above. Since all non-abstract Julia types are final, any supertype is necessarily an abstract type (Any if not specified explicitly).\njulia\u0026gt; abstract type MyInt \u0026lt;: Int32 end ERROR: invalid subtyping in definition of MyInt Stacktrace: [1] ... julia\u0026gt; abstract type MyInt \u0026lt;: supertype(Int32) end julia\u0026gt; subtypetree(Number) Number Complex Real ... Integer Bool Signed ... Int24 ... MyInt ...  Type parameters To various degrees, all of the three major categories of Julia types (primitive, composite, abstract) are available in other dynamic languages, either natively or via some libraries. Julia also offers something that gets it if not into the realm of uber-powerful (and uber-complicated) C++ metaprogramming, then definitely into the realm of Java generics: all three type categories can be further parameterized with other types and values. I\u0026rsquo;ll explore this in a future tutorial.\nType annotations And now to the meat of this tutorial: type annotations. A type annotation in Julia looks like \u0026lt;thing\u0026gt;::\u0026lt;type\u0026gt;4 \u0026ndash; it is an in-place modifier to a \u0026lt;thing\u0026gt; introduced by the :: operator.\nTypeasserts vs variable declarations The way I read the documentation, Julia type annotations can be applied to two types of \u0026lt;thing\u0026gt;s:\n [typeassert] expressions computing a value (and recall that everything in Julia is an expression):\nx = y ::Float64 # promises to the runtime that at this point in the execution 'y' will be a Float64  [variable type declaration] left-hand sides of assignments or declarations that introduce (local) variables:\nx ::Float64 = y # declares a new local 'x', marks it as always containing Float64 values, and initializes with 'y' converted to Float64  This second case also covers typed fields of structs and named tuples:\nstruct Point x ::Float64 # this field will always contain only Float64 values y # this field can contain any Julia value end   The first case is a typeassert. The second kind of annotation marks the name/field to its left as constrained to values compatible with the given type, and also ensures that throughout the variable\u0026rsquo;s scope all subsequent initializations of and assignments to it are filtered through an implicit conversion.\nAs a consequence, there are differences in runtime behavior and the information communicated to the system:\n With a typeassert the compiler will create code that at runtime will check the annotated value for type compatibility and throw a TypeError if the check fails \u0026ndash; but it will not attempt to coerce the computed value to the annotation type in any way. Type-asserted syntax \u0026lt;exp\u0026gt;::T is precisely equivalent to a call, possibly inlined, to typeassert(\u0026lt;exp\u0026gt;, T) followed by making use of the \u0026lt;exp\u0026gt; value. With a variable type declaration any assignment \u0026lt;lhs\u0026gt;::T\u0026nbsp;=\u0026nbsp;\u0026lt;rhs\u0026gt; will effectively translate into \u0026lt;lhs\u0026gt;\u0026nbsp;=\u0026nbsp;convert(T, \u0026lt;rhs\u0026gt;), i.e. contain a call, possibly inlined, to convert(T, \u0026lt;rhs\u0026gt;). And at runtime, every such assignment will attempt to coerce its right hand-side value to T, possibly resulting in a value that\u0026rsquo;s only an approximation. Should this conversion fail, an exception will be thrown:  if no such conversion exists at all, a MethodError is thrown; if T is an Integer (sub)type and cannot represent the expression value, an InexactError is thrown.   To appreciate the difference, compare\njulia\u0026gt; function foo() x ::Float64 = 1 # implies 'convert(Float64, 1)' x, typeof(x) end foo (generic function with 1 method) julia\u0026gt; foo() (1.0, Float64)  with\njulia\u0026gt; function foo() x = 1 ::Float64 # implies 'typeassert(1, Float64)' x, typeof(x) end foo (generic function with 1 method) julia\u0026gt; foo() ERROR: TypeError: in typeassert, expected Float64, got Int64  Literal 1 is of a (machine-dependent) Int type:\njulia\u0026gt; typeof(1) Int64  and even though it can be converted to a Float64 without loss, such a conversion is not even attempted in the second version of foo().\n As I write this, Julia does not yet support type declarations for global variables \u0026ndash; this is the reason I wrapped the above examples into functions.   What about function signatures? Unsurprisingly, it is also possible to type-annotate function arguments and return types:\nfunction bar(x ::Float64) ::Float32 sin(2π * x) end  Function arguments If you\u0026rsquo;re coming from languages like C++ or Java where type conversions can happen as part of argument passing, you might think that x ::Float64 in bar() is like a local (typed) variable declaration, similar to the second case above, perhaps implying a call to something like convert(Float64, x) everywhere before bar() is invoked. That is not the case in Julia: no conversions ever take place as part of Julia function argument passing. In fact, Julia argument type annotations are actually more like those typeasserts: foo(x) will expect x to be a Float64 already.\nThere is a subtle difference from an in-place typeassert, however: with the above definition of bar() there will be no need to generate an implicit call to typeassert() at all because I will only be allowed to call it with Float64s. If I need sin(2π\u0026nbsp;*\u0026nbsp;x) for a Float64 input x, no problem. For any other type5, say, Int64, I will get a flat rejection not because a method call was tried and failed during Int64-to-Float64 input type conversion (TypeError) but because the requisite method (named \u0026ldquo;foo\u0026rdquo; and taking a single argument of type Int64) did not exist (MethodError). And since Float64 is a concrete type and, again, all concrete types are final in Julia, the universe of possible outcomes here shrinks dramatically:\njulia\u0026gt; bar(0.75) -1.0f0 julia\u0026gt; bar(1.) -2.4492937f-16 julia\u0026gt; bar(1) ERROR: MethodError: no method matching bar(::Int64) Closest candidates are: bar(::Float64) at ...  This may seem a little draconian, but it is connected to how Julia\u0026rsquo;s multiple dispatch works and is further ameliorated by Julia\u0026rsquo;s system of promoting function arguments to a common type.\nFunction return types Specifying bar() return type to be Float32 is a way to ensure that value being returned is passed through a convert(Float32, \u0026hellip;). Whether this is desired depends on software design. I can imagine situations where it could be used as a way to safely return \u0026ldquo;special\u0026rdquo; values:\nfunction sqrt_or_nothing(x ::Float64) ::Union{Float64, Nothing} x \u0026lt; 0.0 ? nothing : √x end  julia\u0026gt; @show sqrt_or_nothing(2.0) sqrt_or_nothing(2.0) = 1.4142135623730951 1.414213562373095 julia\u0026gt; @show sqrt_or_nothing(-2.0) sqrt_or_nothing(-2.0) = nothing  Alternatively, it might be easier to reason about your code behavior if most functions are strict about their return value types. Otherwise, it seems like it could be easy in Julia to accidentally return different types along different value return paths, which could cause inefficiencies or maybe even errors downstream:\nfunction bar_clipped(x ::Float64) x \u0026lt; 0.0 ? 0 : sin(2π * x) end  julia\u0026gt; typeof(bar_clipped(0.75)) Float64 julia\u0026gt; typeof(bar_clipped(-0.75)) Int64 # oops, use 0.0 literal instead of 0 above  When are implicit conversions done? Most of the cases of implicit calls to convert(\u0026hellip;) have already been mentioned. Julia documentation offers this complete list:\n  Assigning to an array converts to the array\u0026rsquo;s element type. Assigning to a field of an object converts to the declared type of the field. Constructing an object with new converts to the object\u0026rsquo;s declared field types. Assigning to a variable with a declared type (e.g. local x::T) converts to that type. A function with a declared return type converts its return value to that type. Passing a value to ccall converts it to the corresponding argument type.   Case study: functional OOP Now that we know every function argument in Julia is always associated with a type, that raises a question: is it possible for a Julia runtime session to contain multiple functions that all have the same name and input signature except for types of some (a few) parameters?\nNot only is the answer \u0026ldquo;yes\u0026rdquo;, it is actually kind of like \u0026ldquo;yes, it is meant to happen a lot\u0026ldquo;: Julia thrives on maintaining multiple versions of the \u0026ldquo;same\u0026rdquo; function (called \u0026ldquo;methods\u0026rdquo;) and figuring out which version to invoke for a given set of inputs. Enter \u0026ldquo;multiple dispatch\u0026rdquo;, a core paradigm of Julia programming6. The intuition is that Julia functions are essentially \u0026ldquo;always virtual\u0026rdquo;: unlike other languages where a class method needs to be marked in a special way to support \u0026ldquo;late binding\u0026rdquo; (method dispatch based on runtime, not compile, type of an object), Julia runtime system always dispatches all functions on the concrete runtime types of all their arguments. Because no argument position is \u0026ldquo;special\u0026rdquo; and the method does not \u0026ldquo;belong\u0026rdquo; to any particular parameter type, this form of polymorphism usually opts for language design with standalone functions, that is functions that do not live inside any \u0026ldquo;classes\u0026rdquo;. Some people call such designs \u0026ldquo;functional OOP\u0026rdquo;. It makes a lot of sense for math-style coding due to symmetries in function parameters.\nNow, one way to ease into Julia multiple dispatch is to consider its simplest edge case: single dispatch.\nSerializing Julia objects to JSON I am going to implement a simple serializer of Julia objects to JSON. This will be a toy example, useless in any kind of production setting. My initial point is the \u0026ldquo;interface\u0026rdquo; method to_JSON() calling visit() that in turn starts as a single fallback that always fails:\nfunction to_JSON(io ::IO, obj) visit(obj, io) end function visit(obj, io ::IO) error(\u0026quot;default visit() called for obj type: \u0026quot;, typeof(obj)) end  I am going to anchor my design in the \u0026ldquo;virtual nature\u0026rdquo; of visit(obj,\u0026nbsp;io), with execution routed based on the runtime type of obj. Looking at my Number type trees above, I can see that this overload can cover JSON numbers and booleans:\nfunction visit(obj ::Real, io ::IO) print(io, obj) end  Strings are equally easy, but the method body needs to quote them7, so I need a new method overload:\nfunction visit(obj ::AbstractString, io ::IO) print(io, '\u0026quot;') print(io, obj) print(io, '\u0026quot;') end  Next, for objects that can contain other objects the virtual nature of visit() is critical:\nfunction visit(obj ::AbstractArray, io ::IO) print(io, '[') for i in 1 : length(obj) i \u0026gt; 1 \u0026amp;\u0026amp; print(io, \u0026quot;, \u0026quot;) visit(obj[i], io) end print(io, ']') end function visit(obj ::AbstractDict, io ::IO) print(io, '{') first = true for (k, v) in obj first ? first = false : print(io, \u0026quot;, \u0026quot;) visit(k ::AbstractString, io) # assert that key is a string print(io, \u0026quot; : \u0026quot;) visit(v, io) end print(io, '}') end  The nested visit()s are already virtual, nothing else needs to be done to pick up a particular overload. There are also no \u0026ldquo;if-obj-type-is-\u0026hellip;\u0026rdquo; condition checks \u0026ndash; everything is as clean as in some \u0026ldquo;pure\u0026rdquo; textbook OOP.\nJust a handful of lines of code so far, and yet they already work on a variety of inputs:\njulia\u0026gt; to_JSON(stdout, [1, false, zeros(2), [1.2345, 12, Dict(\u0026quot;a\u0026quot; =\u0026gt; true, \u0026quot;b\u0026quot; =\u0026gt; 2.3, \u0026quot;c\u0026quot; =\u0026gt; [1, 2, 3.4])]]) [1, false, [0.0, 0.0], [1.2345, 12, {\u0026quot;c\u0026quot; : [1.0, 2.0, 3.4], \u0026quot;b\u0026quot; : 2.3, \u0026quot;a\u0026quot; : true}]]  Not bad, looks like valid JSON to me.\nNow, suppose that I would like to extend the set of supported \u0026ldquo;JSON-compatible\u0026rdquo; Julia types to also include tuples. I would like to output them as JSON arrays. All I need to do is add another overload:\nfunction visit(obj ::Tuple, io ::IO) print(io, '[') for i in 1 : length(obj) i \u0026gt; 1 \u0026amp;\u0026amp; print(io, \u0026quot;, \u0026quot;) visit(obj[i], io) end print(io, ']') end  julia\u0026gt; to_JSON(stdout, [(\u0026quot;tuple\u0026quot;, (1, \u0026quot;nested\u0026quot;, \u0026quot;tuple\u0026quot;)) 1, false, zeros(2), [1.2345, 12, Dict(\u0026quot;a\u0026quot; =\u0026gt; true, \u0026quot;b\u0026quot; =\u0026gt; 2.3, \u0026quot;c\u0026quot; =\u0026gt; [1, 2, 3.4])]]) [[\u0026quot;tuple\u0026quot;, [1, \u0026quot;nested\u0026quot;, \u0026quot;tuple\u0026quot;]], 1, false, [0.0, 0.0], [1.2345, 12, {\u0026quot;c\u0026quot; : [1.0, 2.0, 3.4], \u0026quot;b\u0026quot; : 2.3, \u0026quot;a\u0026quot; : true}]]  Note that no earlier visit()s needed to be modified to become \u0026ldquo;aware\u0026rdquo; of this new support for tuples. In other words, a method added later hooks into a set of mutual method invocations coded earlier, with no ostensible \u0026ldquo;recompilation\u0026rdquo;.\nIf you\u0026rsquo;re interested in playing with various design alternatives yourself, you can find this entire example here.\nSummary In summary, I would like to call out some things about Julia typing that make Julia feel different from many languages:\n Julia supports user-definable primitive types which do not need to be \u0026ldquo;boxed\u0026rdquo;. All superclasses are abstract and all concrete classes are final. Mutability is part of type definition, not of argument/variable/field. Multiple dispatch is a primary paradigm (\u0026ldquo;all methods are virtual\u0026rdquo;). This design is consistent with lack of classic \u0026ldquo;objects\u0026rdquo; that forcefully bundle state with behavior. Duck-typing also works in Julia, perhaps even without any performance loss. But certain core Julia features necessitate some static typing.   This is what the public documentation says. Examining boot.jl of my Julia install shows that many of these types are actually implemented in C. ^ As I write this, bit widths must be multiples of 8. ^ Just like classes marked with final keyword in C++ or Java. ^ I use an extra blank before :: but you don\u0026rsquo;t have to. ^ To simply the narrative, I am glossing over possibilities for promotion of x to another type. ^ From what I can tell so far, that is. ^ Note that this toy serializer doesn\u0026rsquo;t bother with backslash escapes, Unicode, etc. ^   ","date":1570320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570320000,"objectID":"50ff1c895ff1937cdfe929286f925115","permalink":"/tutorials/study_julia_with_me/type_annotations/","publishdate":"2019-10-06T00:00:00Z","relpermalink":"/tutorials/study_julia_with_me/type_annotations/","section":"tutorials","summary":"Julia is unusual in that it is a dynamically typed language (meaning you don\u0026rsquo;t have to declare variable types \u0026ldquo;statically\u0026rdquo;, in program text), while at the same time supporting a rich mechanism for communicating type information to the JIT compiler and runtime. Effectively, Julia has both dynamic and static typing. When provided, static type information can have non-trivial impact on Julia code behavior, both in terms of functionality and performance.","tags":null,"title":"Julia type annotations: what Python 'type hints' wish they were","type":"docs"}]